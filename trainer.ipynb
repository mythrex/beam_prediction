{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat './beam_prediction/*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!git clone XXXXX\n",
    "!mv ./beam_prediction/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.10.26)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.10.27-py2.py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (1.0.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (3.15.8)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: Click>=7.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (3.1.14)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Installing collected packages: wandb\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.10.26\n",
      "    Uninstalling wandb-0.10.26:\n",
      "      Successfully uninstalled wandb-0.10.26\n",
      "Successfully installed wandb-0.10.27\n",
      "Requirement already satisfied: pytorch-lightning in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (1.2.8)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (2.4.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (4.60.0)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (2021.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.4 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (1.20.2)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (5.3.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (49.6.0.post20210108)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.15.8)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.29.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.37.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (1.26.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
      "Requirement already satisfied: albumentations in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.5.2)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (1.20.2)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (0.18.1)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (1.6.2)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (3.4.1)\n",
      "Requirement already satisfied: Shapely in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (8.2.0)\n",
      "Requirement already satisfied: imageio in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\n",
      "Requirement already satisfied: opencv-python in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2021.4.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.17.0)\n",
      "Requirement already satisfied: torchmetrics in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torchmetrics) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (1.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb --upgrade\n",
    "!pip install pytorch-lightning\n",
    "!pip install albumentations\n",
    "!pip install python-dotenv\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-25 16:09:04--  https://docs.google.com/uc?export=download&confirm=UV8V&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5\n",
      "Resolving docs.google.com (docs.google.com)... 142.250.182.206, 2404:6800:4009:812::200e\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.182.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ifdabl8e5eng3u8oup5de01ncp4ue0ij/1488ghcdheqo554a6i043cov81t7g75i/1619366925000/18183255587859120126/14860611907455554416Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download [following]\n",
      "--2021-04-25 16:09:05--  https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ifdabl8e5eng3u8oup5de01ncp4ue0ij/1488ghcdheqo554a6i043cov81t7g75i/1619366925000/18183255587859120126/14860611907455554416Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download\n",
      "Resolving doc-08-6k-docs.googleusercontent.com (doc-08-6k-docs.googleusercontent.com)... 142.250.183.65, 2404:6800:4009:820::2001\n",
      "Connecting to doc-08-6k-docs.googleusercontent.com (doc-08-6k-docs.googleusercontent.com)|142.250.183.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=2heg33pkk05dq&continue=https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ifdabl8e5eng3u8oup5de01ncp4ue0ij/1488ghcdheqo554a6i043cov81t7g75i/1619366925000/18183255587859120126/14860611907455554416Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e%3Ddownload&hash=6rtu0335i78suahh325ref8q1vnb5gca [following]\n",
      "--2021-04-25 16:09:05--  https://docs.google.com/nonceSigner?nonce=2heg33pkk05dq&continue=https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ifdabl8e5eng3u8oup5de01ncp4ue0ij/1488ghcdheqo554a6i043cov81t7g75i/1619366925000/18183255587859120126/14860611907455554416Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e%3Ddownload&hash=6rtu0335i78suahh325ref8q1vnb5gca\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.182.206|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ifdabl8e5eng3u8oup5de01ncp4ue0ij/1488ghcdheqo554a6i043cov81t7g75i/1619366925000/18183255587859120126/14860611907455554416Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download&nonce=2heg33pkk05dq&user=14860611907455554416Z&hash=6st39gr5o51t6i3f60s5a5moslom8b0k [following]\n",
      "--2021-04-25 16:09:06--  https://doc-08-6k-docs.googleusercontent.com/docs/securesc/ifdabl8e5eng3u8oup5de01ncp4ue0ij/1488ghcdheqo554a6i043cov81t7g75i/1619366925000/18183255587859120126/14860611907455554416Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download&nonce=2heg33pkk05dq&user=14860611907455554416Z&hash=6st39gr5o51t6i3f60s5a5moslom8b0k\n",
      "Connecting to doc-08-6k-docs.googleusercontent.com (doc-08-6k-docs.googleusercontent.com)|142.250.183.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘download.tar.gz’\n",
      "\n",
      "download.tar.gz         [              <=>   ] 821.66M  40.4MB/s               ^C\n",
      "./Dataset_-17.3375dB/\n",
      "./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy\n",
      "./Dataset_-17.3375dB/-17.3375dB_highFreqChVal.npy\n",
      "tar: Unexpected EOF in archive\n",
      "tar: rmtlseek not stopped at a record boundary\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "# https://drive.google.com/file/d//view?usp=sharing\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5\" -O \"download.tar.gz\" && rm -rf /tmp/cookies.txt\n",
    "!tar -xvf download.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pdb\n",
    "\n",
    "# def calc_acc(pred: torch.tensor, y: torch.tensor, num_classes: int, return_class_wise_acc: bool = False):    pred = pred.argmax(1)\n",
    "#     class_wise_acc = []\n",
    "#     for i in range(num_classes):\n",
    "#         tp = ((pred == i) & (y == i)).sum().float()\n",
    "#         tn = ((pred != i) & (y != i)).sum().float()\n",
    "#         fp = ((pred == i) & (y != i)).sum().float()\n",
    "#         fn = ((pred != i) & (y == i)).sum().float()\n",
    "#         acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "#         class_wise_acc.append(acc)\n",
    "    \n",
    "# #     pdb.set_trace()\n",
    "#     class_wise_acc = torch.Tensor(class_wise_acc)\n",
    "#     if return_class_wise_acc:\n",
    "#         return class_wise_acc\n",
    "#     return class_wise_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamPredictionDataset(Dataset):\n",
    "    \"\"\"Beam Prediction dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str,\n",
    "                 label_file_path: str,\n",
    "                 reshape: bool=False,\n",
    "                 add_padding: bool=True,\n",
    "                 transforms: Optional[transforms.Compose] = None,\n",
    "                 preprocessing_fn: Optional[transforms.Compose] = None) -> None:\n",
    "        \"\"\"\n",
    "        Init the Dataset\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.label_file_path = label_file_path\n",
    "        \n",
    "        self.data = np.load(file_path)\n",
    "        self.data = self.data.transpose((1, 0))\n",
    "        \n",
    "        self.label = np.load(label_file_path)\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        assert len(self.label) == len(self.data)\n",
    "        # reshape is true\n",
    "        # num_users x num_channels x num_antennas x real/imaginary\n",
    "        if reshape:\n",
    "            self.data = self.data.reshape((2, 4, 32, -1))\n",
    "            self.data = self.data.transpose((3, 2, 1, 0))\n",
    "            \n",
    "        if add_padding:\n",
    "            self.data = np.pad(self.data, \n",
    "                               ((0, 0), (0, 0), (0, 1), (0, 1)),\n",
    "                              'constant',\n",
    "                              constant_values=0)\n",
    "            \n",
    "        self.preprocessing_fn = preprocessing_fn\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Gets an item from dataset\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        user_data = self.data[idx]\n",
    "        \n",
    "        if self.preprocessing_fn is not None:\n",
    "            user_data = self.preprocessing_fn(user_data)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            user_data = self.transforms(user_data)\n",
    "        \n",
    "        label = self.label[idx]\n",
    "        label = torch.Tensor(label).type(torch.int64) - 1\n",
    "        return user_data, label\n",
    "    \n",
    "def transform(x: np.array, \n",
    "              mean: float = -2.19e-6, \n",
    "              std: float = 0.1558) -> torch.Tensor:\n",
    "    # mean normalize\n",
    "    x -= mean\n",
    "    x /= std\n",
    "    x = torch.Tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# example ds\n",
    "\n",
    "ds = BeamPredictionDataset(\n",
    "    file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
    "    label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
    "    reshape=True,\n",
    "    transforms=transform\n",
    ")\n",
    "it = iter(ds)\n",
    "out = next(it)\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from argparse import ArgumentParser\n",
    "from typing import Tuple\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pdb\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_ch: int, \n",
    "                 out_ch: int, \n",
    "                 dropout_prob: float) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=(5, 3), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout_prob),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x1 = self.conv(x)\n",
    "        return x1 + x\n",
    "\n",
    "class BeamClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams) -> None:\n",
    "        \"\"\"\n",
    "        Downloading Backbone and defining structure of model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # args from argparser\n",
    "        self.hparams = hparams\n",
    "        in_ch = self.hparams.in_ch\n",
    "        out_ch = self.hparams.out_ch\n",
    "        mid_ch = self.hparams.mid_ch\n",
    "        \n",
    "        self.first_conv = nn.Sequential(\n",
    "            # first \n",
    "            nn.Conv2d(self.hparams.in_ch, mid_ch, kernel_size=(5, 3), padding=(2, 1)),\n",
    "            nn.BatchNorm2d(mid_ch),\n",
    "            nn.ReLU(),\n",
    "#             nn.Dropout2d(self.hparams.dropout_prob),\n",
    "#             nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        layers = nn.ModuleList()\n",
    "        for i in range(self.hparams.num_layers):\n",
    "            layers.append(\n",
    "                ResidualBlock(mid_ch, mid_ch, self.hparams.dropout_prob)\n",
    "            )\n",
    "           # layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "            \n",
    "        self.adap_avg = nn.AdaptiveAvgPool2d(2)\n",
    "        \n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2*2*mid_ch, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.hparams.dropout_prob),\n",
    "            \n",
    "            nn.Linear(2048, self.hparams.out_ch)\n",
    "        )\n",
    "        \n",
    "        self._acc_metric = Accuracy()\n",
    "        self._top2_acc_metric = Accuracy(top_k=2)\n",
    "        self._precision = Precision(average='macro', \n",
    "                                   num_classes=self.hparams.out_ch)\n",
    "        self._recall = Recall(average='macro', \n",
    "                             num_classes=self.hparams.out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward step of model.\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "#         first Conv\n",
    "        x = self.first_conv(x)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        x = self.adap_avg(x)\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.view(b, -1) \n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def loss_fn(self, pred: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Loss function used for model\"\"\"\n",
    "        y_sq = y.squeeze(-1)\n",
    "        loss = F.cross_entropy(pred, y_sq)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim:\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        opt = torch.optim.Adam(self.parameters(),\n",
    "                               lr=self.hparams.learning_rate,\n",
    "                              weight_decay= self.hparams.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            opt, self.hparams.max_nb_epochs, self.hparams.learning_rate)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "#         lr_scheduler = None\n",
    "        return [opt], [lr_scheduler]\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the data loader for training data\"\"\"\n",
    "        # REQUIRED\n",
    "        return DataLoader(BeamPredictionDataset(\n",
    "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
    "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
    "                                reshape=True,\n",
    "                                transforms=transform\n",
    "                            ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers,\n",
    "                          shuffle=True)\n",
    "\n",
    "    def training_step(self, batch: list, batch_idx: int) -> dict:\n",
    "        \"\"\"Backward step of model\"\"\"\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        pred = F.softmax(pred, dim=-1)\n",
    "        y_sq = y.squeeze(-1)\n",
    "        \n",
    "        # metrics\n",
    "        acc = self._acc_metric(pred, y_sq)\n",
    "        prec = self._precision(pred, y_sq)\n",
    "        rec = self._recall(pred, y_sq)\n",
    "        \n",
    "        if self.lr_scheduler is not None:\n",
    "            lr = self.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        if(batch_idx % self.hparams.wandb_log_num_iter == 0):\n",
    "            wandb.log({\n",
    "                'train_loss': loss,\n",
    "            })\n",
    "            \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'train_acc': acc,\n",
    "            'train_prec': prec,\n",
    "            'train_rec': rec\n",
    "        }\n",
    "    \n",
    "    def training_epoch_end(self, outputs: list) -> None:\n",
    "        acc = torch.stack([x['train_acc'] for x in outputs]).mean()\n",
    "        prec = torch.stack([x['train_prec'] for x in outputs]).mean()\n",
    "        rec = torch.stack([x['train_rec'] for x in outputs]).mean()\n",
    "        \n",
    "        if self.lr_scheduler is not None:\n",
    "            lr = self.lr_scheduler.get_last_lr()[0]\n",
    "        else:\n",
    "            lr = self.hparams.learning_rate\n",
    "        logs = {\n",
    "            'lr': lr,\n",
    "            'train_acc': acc,\n",
    "            'train_prec': prec,\n",
    "            'train_rec': rec\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "        self.log_dict(logs)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the data loader for validation data\"\"\"\n",
    "        # OPTIONAL\n",
    "        return DataLoader(BeamPredictionDataset(\n",
    "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpVal.npy',\n",
    "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelVal.npy',\n",
    "                                reshape=True,\n",
    "                                transforms=transform\n",
    "                            ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers\n",
    "                         )\n",
    "\n",
    "    def validation_step(self, batch: list, batch_idx: torch.Tensor) -> dict:\n",
    "        \"\"\"Validation step to be carried out on validation data.\"\"\"\n",
    "        # REQUIRED\n",
    "        # pdb.set_trace()\n",
    "        x, y = batch\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        pred = F.softmax(pred, dim=-1)\n",
    "        y_sq = y.squeeze(-1)\n",
    "        \n",
    "        acc = self._acc_metric(pred, y_sq)\n",
    "        prec = self._precision(pred, y_sq)\n",
    "        rec = self._recall(pred, y_sq)\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': acc,\n",
    "            'val_prec': prec,\n",
    "            'val_rec': rec\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list) -> None:\n",
    "        \"\"\"Use results from each validation step to generate validation stats at epoch end\"\"\"\n",
    "        val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        val_prec = torch.stack([x['val_prec'] for x in outputs]).mean()\n",
    "        val_rec = torch.stack([x['val_rec'] for x in outputs]).mean()\n",
    "        \n",
    "        logs = {\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_prec': val_prec,\n",
    "            'val_rec': val_rec\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "        self.log_dict(logs)\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the data loader for test data\"\"\"\n",
    "        print(\"Test Dataloader\")\n",
    "        # OPTIONAL\n",
    "        return DataLoader(BeamPredictionDataset(\n",
    "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpVal.npy',\n",
    "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelVal.npy',\n",
    "                                reshape=True,\n",
    "                                transforms=transform\n",
    "                            ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          num_workers=self.hparams.num_workers\n",
    "                         )\n",
    "    \n",
    "    def test_step(self, batch: list, batch_idx: torch.Tensor) -> dict:\n",
    "        \"\"\"Validation step to be carried out on validation data.\"\"\"\n",
    "        # REQUIRED\n",
    "#         pdb.set_trace()\n",
    "        x, y = batch\n",
    "        \n",
    "        pred = self.forward(x)        \n",
    "        pred = F.softmax(pred, dim=-1)\n",
    "        y_sq = y.squeeze(-1)\n",
    "        logs = {\n",
    "            'pred': pred,\n",
    "            'ground_truth': y_sq\n",
    "        }\n",
    "#         self.log_dict(logs)\n",
    "        return logs\n",
    "    \n",
    "    def test_epoch_end(self, outputs: list) -> None:\n",
    "        \"\"\"Use results from each validation step to generate validation stats at epoch end\"\"\"\n",
    "        pred = torch.cat([x['pred'] for x in outputs], dim=0)\n",
    "        ground_truth = torch.cat([x['ground_truth'] for x in outputs], dim=0)\n",
    "\n",
    "        top1_acc = self._acc_metric(pred, ground_truth)\n",
    "        top2_acc = self._top2_acc_metric(pred, ground_truth)\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        conf_matrix = confusion_matrix(ground_truth.tolist(), \n",
    "                                       pred.argmax(-1).tolist(), \n",
    "                                       labels=list(range(self.hparams.out_ch)))\n",
    "        df_cm = pd.DataFrame(conf_matrix, index = [str(i) for i in range(self.hparams.out_ch)],\n",
    "                  columns = [str(i) for i in range(self.hparams.out_ch)])\n",
    "        \n",
    "        plt.figure(figsize = (50,50))\n",
    "        ax = sns.heatmap(df_cm, annot=True)\n",
    "        logs = {\n",
    "            'top1_acc': top1_acc,\n",
    "            'top2_acc': top2_acc,\n",
    "            'conf_matrix': wandb.Image(ax)\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "    \n",
    "\n",
    "    def load_encoder_weights(self) -> None:\n",
    "        \"\"\"Loads encoder weights from ckpt\"\"\"\n",
    "        ckpt = torch.load(self.hparams.encoder_ckpt_path)\n",
    "        pretrained_dict = ckpt['state_dict']\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if (\n",
    "            'encoder' in k) and (k in model_dict)}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def load_model_weights_from_ckpt(self) -> None:\n",
    "        \"\"\"Load model weights to model on cpu\"\"\"\n",
    "        ckpt = torch.load(self.hparams.model_ckpt_path,\n",
    "                          map_location=torch.device('cpu'))\n",
    "        pretrained_dict = ckpt['state_dict']\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in pretrained_dict.items() if (k in model_dict)}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        \n",
    "    def _get_learning_rate(self) -> float:\n",
    "        i = 0\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            if i == 0:\n",
    "                learning_rate = param_group[\"lr\"]\n",
    "            else:\n",
    "                if learning_rate != param_group[\"lr\"]:\n",
    "                    raise ValueError(\n",
    "                        \"different param groups have different lr\")\n",
    "        return learning_rate\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n",
    "        \"\"\"\n",
    "        Specify the hyperparams for this LightningModule\n",
    "        \"\"\"\n",
    "        # MODEL specific arguments\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--learning_rate', default=0.02, type=float)\n",
    "        parser.add_argument('--batch_size', default=32, type=int)\n",
    "        parser.add_argument('--in_ch', default=256, type=int)\n",
    "        parser.add_argument('--out_ch', default=64, type=int)\n",
    "        parser.add_argument('--num_workers', default=1, type=int)\n",
    "        parser.add_argument('--max_nb_epochs', default=1, type=int)\n",
    "        parser.add_argument('--dropout_prob', default=0.5, type=float)\n",
    "        parser.add_argument('--num_layers', default=8, type=int)\n",
    "        parser.add_argument('--mid_ch', default=128, type=int)\n",
    "        parser.add_argument('--weight_decay', default=0.0, type=float)\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.4446, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser, Namespace\n",
    "args_str = [\n",
    "        # model related args\n",
    "        '--max_nb_epochs=1',\n",
    "        '--learning_rate=1e-3',\n",
    "        '--batch_size=16',\n",
    "        '--in_ch=32',\n",
    "        '--out_ch=64',\n",
    "        '--num_workers=4'\n",
    "]\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser = BeamClassifier.add_model_specific_args(parser)\n",
    "args= parser.parse_args(args_str)\n",
    "\n",
    "model = BeamClassifier(args)\n",
    "train_dl = model.train_dataloader()\n",
    "it = iter(train_dl)\n",
    "x, y = next(it)\n",
    "\n",
    "\n",
    "pred = model(x)\n",
    "loss = model.loss_fn(pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_str = ['--tpu_cores=0',\n",
    "        '--progress_bar_refresh_rate=20',\n",
    "        '--wandb_run_name=CNN Avg Pool',\n",
    "        '--wandb_project_name=Beam Prediction',\n",
    "        '--wandb_log_num_iter=1',\n",
    "        '--gpus=1',\n",
    "        # model related args\n",
    "        '--max_nb_epochs=100',\n",
    "        '--learning_rate=5e-3',\n",
    "        '--batch_size=256',\n",
    "        '--in_ch=32',\n",
    "        '--out_ch=64',\n",
    "        '--num_workers=4',\n",
    "        '--dropout_prob=0.1',\n",
    "        '--num_layers=3',\n",
    "        '--mid_ch=256',\n",
    "        '--weight_decay=0.00'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msagarkaushik98\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">CNN Avg Pool</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/2dlzohnk\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/2dlzohnk</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/619/wandb/run-20210425_161010-2dlzohnk</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type              | Params\n",
      "-------------------------------------------------------\n",
      "0 | first_conv       | Sequential        | 123 K \n",
      "1 | layers           | Sequential        | 3.0 M \n",
      "2 | adap_avg         | AdaptiveAvgPool2d | 0     \n",
      "3 | classifier       | Sequential        | 2.2 M \n",
      "4 | _acc_metric      | Accuracy          | 0     \n",
      "5 | _top2_acc_metric | Accuracy          | 0     \n",
      "6 | _precision       | Precision         | 0     \n",
      "7 | _recall          | Recall            | 0     \n",
      "-------------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.238    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/426 [00:00<02:04,  3.40it/s, loss=4.94, v_num=]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  70%|███████   | 299/426 [00:13<00:05, 21.97it/s, loss=4.01, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  71%|███████   | 302/426 [00:13<00:05, 21.76it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  73%|███████▎  | 310/426 [00:13<00:05, 22.16it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  75%|███████▍  | 318/426 [00:14<00:04, 22.54it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  77%|███████▋  | 326/426 [00:14<00:04, 22.70it/s, loss=4.01, v_num=]\n",
      "Validating:  22%|██▏       | 28/128 [00:00<00:02, 41.75it/s]\u001b[A\n",
      "Epoch 0:  78%|███████▊  | 334/426 [00:14<00:03, 23.06it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  80%|████████  | 342/426 [00:14<00:03, 23.44it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  82%|████████▏ | 350/426 [00:14<00:03, 23.80it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  84%|████████▍ | 358/426 [00:14<00:02, 24.13it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  86%|████████▌ | 366/426 [00:14<00:02, 24.47it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  88%|████████▊ | 374/426 [00:15<00:02, 24.80it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  90%|████████▉ | 382/426 [00:15<00:01, 25.11it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  92%|█████████▏| 390/426 [00:15<00:01, 25.40it/s, loss=4.01, v_num=]\n",
      "Validating:  72%|███████▏  | 92/128 [00:01<00:00, 60.42it/s]\u001b[A\n",
      "Epoch 0:  93%|█████████▎| 398/426 [00:15<00:01, 25.70it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  95%|█████████▌| 406/426 [00:15<00:00, 26.02it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  97%|█████████▋| 414/426 [00:15<00:00, 26.33it/s, loss=4.01, v_num=]\n",
      "Epoch 0:  99%|█████████▉| 422/426 [00:15<00:00, 26.65it/s, loss=4.01, v_num=]\n",
      "Epoch 0: 100%|██████████| 426/426 [00:15<00:00, 26.70it/s, loss=4.01, v_num=]\n",
      "Epoch 1:  70%|██████▉   | 298/426 [00:13<00:05, 22.11it/s, loss=4, v_num=]   \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  71%|███████▏  | 304/426 [00:13<00:05, 22.02it/s, loss=4, v_num=]\n",
      "Epoch 1:  73%|███████▎  | 312/426 [00:13<00:05, 22.40it/s, loss=4, v_num=]\n",
      "Epoch 1:  75%|███████▌  | 320/426 [00:14<00:04, 22.75it/s, loss=4, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 47.70it/s]\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 328/426 [00:14<00:04, 23.12it/s, loss=4, v_num=]\n",
      "Epoch 1:  79%|███████▉  | 336/426 [00:14<00:03, 23.48it/s, loss=4, v_num=]\n",
      "Epoch 1:  81%|████████  | 344/426 [00:14<00:03, 23.85it/s, loss=4, v_num=]\n",
      "Epoch 1:  83%|████████▎ | 352/426 [00:14<00:03, 24.21it/s, loss=4, v_num=]\n",
      "Epoch 1:  85%|████████▍ | 360/426 [00:14<00:02, 24.57it/s, loss=4, v_num=]\n",
      "Epoch 1:  86%|████████▋ | 368/426 [00:14<00:02, 24.92it/s, loss=4, v_num=]\n",
      "Epoch 1:  88%|████████▊ | 376/426 [00:14<00:01, 25.26it/s, loss=4, v_num=]\n",
      "Epoch 1:  90%|█████████ | 384/426 [00:15<00:01, 25.57it/s, loss=4, v_num=]\n",
      "Validating:  67%|██████▋   | 86/128 [00:01<00:00, 64.80it/s]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 392/426 [00:15<00:01, 25.88it/s, loss=4, v_num=]\n",
      "Epoch 1:  94%|█████████▍| 400/426 [00:15<00:00, 26.21it/s, loss=4, v_num=]\n",
      "Epoch 1:  96%|█████████▌| 408/426 [00:15<00:00, 26.55it/s, loss=4, v_num=]\n",
      "Epoch 1:  98%|█████████▊| 416/426 [00:15<00:00, 26.88it/s, loss=4, v_num=]\n",
      "Epoch 1: 100%|██████████| 426/426 [00:15<00:00, 27.15it/s, loss=4, v_num=]\n",
      "Epoch 2:  70%|██████▉   | 298/426 [00:13<00:05, 22.10it/s, loss=3.99, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  71%|███████▏  | 304/426 [00:13<00:05, 22.02it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  73%|███████▎  | 312/426 [00:13<00:05, 22.40it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  75%|███████▌  | 320/426 [00:14<00:04, 22.79it/s, loss=3.99, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 52.04it/s]\u001b[A\n",
      "Epoch 2:  77%|███████▋  | 328/426 [00:14<00:04, 23.16it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  79%|███████▉  | 336/426 [00:14<00:03, 23.52it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  81%|████████  | 344/426 [00:14<00:03, 23.85it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  83%|████████▎ | 352/426 [00:14<00:03, 24.21it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  85%|████████▍ | 360/426 [00:14<00:02, 24.56it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  86%|████████▋ | 368/426 [00:14<00:02, 24.91it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  88%|████████▊ | 376/426 [00:14<00:01, 25.26it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  90%|█████████ | 384/426 [00:14<00:01, 25.61it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  92%|█████████▏| 392/426 [00:15<00:01, 25.95it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  94%|█████████▍| 400/426 [00:15<00:00, 26.29it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  96%|█████████▌| 408/426 [00:15<00:00, 26.62it/s, loss=3.99, v_num=]\n",
      "Epoch 2:  98%|█████████▊| 416/426 [00:15<00:00, 26.93it/s, loss=3.99, v_num=]\n",
      "Epoch 2: 100%|█████████▉| 424/426 [00:15<00:00, 27.26it/s, loss=3.99, v_num=]\n",
      "Epoch 2: 100%|██████████| 426/426 [00:15<00:00, 27.22it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  70%|██████▉   | 298/426 [00:13<00:05, 22.00it/s, loss=3.99, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  71%|███████▏  | 304/426 [00:13<00:05, 21.92it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  73%|███████▎  | 312/426 [00:13<00:05, 22.31it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  75%|███████▌  | 320/426 [00:14<00:04, 22.68it/s, loss=3.99, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 49.98it/s]\u001b[A\n",
      "Epoch 3:  77%|███████▋  | 328/426 [00:14<00:04, 23.05it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  79%|███████▉  | 336/426 [00:14<00:03, 23.43it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  81%|████████  | 344/426 [00:14<00:03, 23.80it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  83%|████████▎ | 352/426 [00:14<00:03, 24.15it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  85%|████████▍ | 360/426 [00:14<00:02, 24.47it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  86%|████████▋ | 368/426 [00:14<00:02, 24.81it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  88%|████████▊ | 376/426 [00:14<00:01, 25.15it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  90%|█████████ | 384/426 [00:15<00:01, 25.49it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  92%|█████████▏| 392/426 [00:15<00:01, 25.81it/s, loss=3.99, v_num=]\n",
      "Validating:  73%|███████▎  | 94/128 [00:01<00:00, 65.34it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 400/426 [00:15<00:00, 26.12it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  96%|█████████▌| 408/426 [00:15<00:00, 26.43it/s, loss=3.99, v_num=]\n",
      "Epoch 3:  98%|█████████▊| 416/426 [00:15<00:00, 26.73it/s, loss=3.99, v_num=]\n",
      "Epoch 3: 100%|██████████| 426/426 [00:15<00:00, 27.03it/s, loss=3.99, v_num=]\n",
      "Epoch 4:  70%|██████▉   | 298/426 [00:13<00:05, 21.92it/s, loss=3.97, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  71%|███████▏  | 304/426 [00:13<00:05, 21.84it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  73%|███████▎  | 312/426 [00:14<00:05, 22.23it/s, loss=3.97, v_num=]\n",
      "Validating:  12%|█▏        | 15/128 [00:00<00:02, 42.08it/s]\u001b[A\n",
      "Epoch 4:  75%|███████▌  | 320/426 [00:14<00:04, 22.58it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  77%|███████▋  | 328/426 [00:14<00:04, 22.94it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  79%|███████▉  | 336/426 [00:14<00:03, 23.28it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  81%|████████  | 344/426 [00:14<00:03, 23.64it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  83%|████████▎ | 352/426 [00:14<00:03, 23.99it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  85%|████████▍ | 360/426 [00:14<00:02, 24.36it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  86%|████████▋ | 368/426 [00:14<00:02, 24.71it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  88%|████████▊ | 376/426 [00:15<00:01, 25.04it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  90%|█████████ | 384/426 [00:15<00:01, 25.36it/s, loss=3.97, v_num=]\n",
      "Validating:  67%|██████▋   | 86/128 [00:01<00:00, 65.33it/s]\u001b[A\n",
      "Epoch 4:  92%|█████████▏| 392/426 [00:15<00:01, 25.69it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  94%|█████████▍| 400/426 [00:15<00:01, 26.00it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  96%|█████████▌| 408/426 [00:15<00:00, 26.31it/s, loss=3.97, v_num=]\n",
      "Epoch 4:  98%|█████████▊| 416/426 [00:15<00:00, 26.62it/s, loss=3.97, v_num=]\n",
      "Epoch 4: 100%|██████████| 426/426 [00:15<00:00, 26.92it/s, loss=3.97, v_num=]\n",
      "Epoch 5:  70%|██████▉   | 298/426 [00:13<00:05, 22.16it/s, loss=3.94, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  71%|███████▏  | 304/426 [00:13<00:05, 22.08it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  73%|███████▎  | 312/426 [00:13<00:05, 22.46it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  75%|███████▌  | 320/426 [00:14<00:04, 22.85it/s, loss=3.94, v_num=]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 50.66it/s]\u001b[A\n",
      "Epoch 5:  77%|███████▋  | 328/426 [00:14<00:04, 23.20it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  79%|███████▉  | 336/426 [00:14<00:03, 23.57it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  81%|████████  | 344/426 [00:14<00:03, 23.93it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  83%|████████▎ | 352/426 [00:14<00:03, 24.28it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  85%|████████▍ | 360/426 [00:14<00:02, 24.64it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  86%|████████▋ | 368/426 [00:14<00:02, 24.99it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  88%|████████▊ | 376/426 [00:14<00:01, 25.32it/s, loss=3.94, v_num=]\n",
      "Validating:  61%|██████    | 78/128 [00:01<00:00, 65.36it/s]\u001b[A\n",
      "Epoch 5:  90%|█████████ | 384/426 [00:14<00:01, 25.63it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  92%|█████████▏| 392/426 [00:15<00:01, 25.96it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  94%|█████████▍| 400/426 [00:15<00:00, 26.27it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  96%|█████████▌| 408/426 [00:15<00:00, 26.59it/s, loss=3.94, v_num=]\n",
      "Epoch 5:  98%|█████████▊| 416/426 [00:15<00:00, 26.91it/s, loss=3.94, v_num=]\n",
      "Epoch 5: 100%|█████████▉| 424/426 [00:15<00:00, 27.22it/s, loss=3.94, v_num=]\n",
      "Epoch 5: 100%|██████████| 426/426 [00:15<00:00, 27.20it/s, loss=3.94, v_num=]\n",
      "Epoch 6:  70%|██████▉   | 298/426 [00:13<00:05, 22.04it/s, loss=3.86, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  71%|███████▏  | 304/426 [00:13<00:05, 21.91it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  73%|███████▎  | 312/426 [00:13<00:05, 22.31it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  75%|███████▌  | 320/426 [00:14<00:04, 22.71it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  77%|███████▋  | 328/426 [00:14<00:04, 23.08it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  79%|███████▉  | 336/426 [00:14<00:03, 23.46it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  81%|████████  | 344/426 [00:14<00:03, 23.83it/s, loss=3.86, v_num=]\n",
      "Validating:  36%|███▌      | 46/128 [00:00<00:01, 63.55it/s]\u001b[A\n",
      "Epoch 6:  83%|████████▎ | 352/426 [00:14<00:03, 24.17it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  85%|████████▍ | 360/426 [00:14<00:02, 24.51it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  86%|████████▋ | 368/426 [00:14<00:02, 24.86it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  88%|████████▊ | 376/426 [00:14<00:01, 25.21it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  90%|█████████ | 384/426 [00:15<00:01, 25.55it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  92%|█████████▏| 392/426 [00:15<00:01, 25.89it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  94%|█████████▍| 400/426 [00:15<00:00, 26.21it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  96%|█████████▌| 408/426 [00:15<00:00, 26.54it/s, loss=3.86, v_num=]\n",
      "Epoch 6:  98%|█████████▊| 416/426 [00:15<00:00, 26.87it/s, loss=3.86, v_num=]\n",
      "Epoch 6: 100%|█████████▉| 424/426 [00:15<00:00, 27.19it/s, loss=3.86, v_num=]\n",
      "Epoch 6: 100%|██████████| 426/426 [00:15<00:00, 27.17it/s, loss=3.86, v_num=]\n",
      "Epoch 7:  70%|██████▉   | 298/426 [00:13<00:05, 22.05it/s, loss=3.67, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  71%|███████▏  | 304/426 [00:13<00:05, 21.96it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  73%|███████▎  | 312/426 [00:13<00:05, 22.33it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  75%|███████▌  | 320/426 [00:14<00:04, 22.69it/s, loss=3.67, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 47.43it/s]\u001b[A\n",
      "Epoch 7:  77%|███████▋  | 328/426 [00:14<00:04, 23.03it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  79%|███████▉  | 336/426 [00:14<00:03, 23.39it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  81%|████████  | 344/426 [00:14<00:03, 23.75it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  83%|████████▎ | 352/426 [00:14<00:03, 24.11it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  85%|████████▍ | 360/426 [00:14<00:02, 24.46it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  86%|████████▋ | 368/426 [00:14<00:02, 24.81it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  88%|████████▊ | 376/426 [00:14<00:01, 25.14it/s, loss=3.67, v_num=]\n",
      "Validating:  61%|██████    | 78/128 [00:01<00:00, 66.22it/s]\u001b[A\n",
      "Epoch 7:  90%|█████████ | 384/426 [00:15<00:01, 25.45it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  92%|█████████▏| 392/426 [00:15<00:01, 25.78it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  94%|█████████▍| 400/426 [00:15<00:00, 26.11it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  96%|█████████▌| 408/426 [00:15<00:00, 26.44it/s, loss=3.67, v_num=]\n",
      "Epoch 7:  98%|█████████▊| 416/426 [00:15<00:00, 26.75it/s, loss=3.67, v_num=]\n",
      "Epoch 7: 100%|██████████| 426/426 [00:15<00:00, 27.04it/s, loss=3.67, v_num=]\n",
      "Epoch 8:  70%|██████▉   | 298/426 [00:13<00:05, 22.26it/s, loss=3.37, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  71%|███████▏  | 304/426 [00:13<00:05, 22.13it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  73%|███████▎  | 312/426 [00:13<00:05, 22.52it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  75%|███████▌  | 320/426 [00:13<00:04, 22.92it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  77%|███████▋  | 328/426 [00:14<00:04, 23.31it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  79%|███████▉  | 336/426 [00:14<00:03, 23.67it/s, loss=3.37, v_num=]\n",
      "Validating:  30%|██▉       | 38/128 [00:00<00:01, 60.40it/s]\u001b[A\n",
      "Epoch 8:  81%|████████  | 344/426 [00:14<00:03, 24.04it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  83%|████████▎ | 352/426 [00:14<00:03, 24.39it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  85%|████████▍ | 360/426 [00:14<00:02, 24.75it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  86%|████████▋ | 368/426 [00:14<00:02, 25.10it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  88%|████████▊ | 376/426 [00:14<00:01, 25.45it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  90%|█████████ | 384/426 [00:14<00:01, 25.80it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  92%|█████████▏| 392/426 [00:15<00:01, 26.13it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  94%|█████████▍| 400/426 [00:15<00:00, 26.44it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  96%|█████████▌| 408/426 [00:15<00:00, 26.74it/s, loss=3.37, v_num=]\n",
      "Epoch 8:  98%|█████████▊| 416/426 [00:15<00:00, 27.04it/s, loss=3.37, v_num=]\n",
      "Epoch 8: 100%|█████████▉| 424/426 [00:15<00:00, 27.36it/s, loss=3.37, v_num=]\n",
      "Epoch 8: 100%|██████████| 426/426 [00:15<00:00, 27.33it/s, loss=3.37, v_num=]\n",
      "Epoch 9:  70%|██████▉   | 298/426 [00:13<00:05, 22.12it/s, loss=2.93, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  71%|███████▏  | 304/426 [00:13<00:05, 22.05it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  73%|███████▎  | 312/426 [00:13<00:05, 22.44it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  75%|███████▌  | 320/426 [00:14<00:04, 22.80it/s, loss=2.93, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 49.43it/s]\u001b[A\n",
      "Epoch 9:  77%|███████▋  | 328/426 [00:14<00:04, 23.17it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  79%|███████▉  | 336/426 [00:14<00:03, 23.55it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  81%|████████  | 344/426 [00:14<00:03, 23.91it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  83%|████████▎ | 352/426 [00:14<00:03, 24.24it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  85%|████████▍ | 360/426 [00:14<00:02, 24.57it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  86%|████████▋ | 368/426 [00:14<00:02, 24.90it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  88%|████████▊ | 376/426 [00:14<00:01, 25.22it/s, loss=2.93, v_num=]\n",
      "Validating:  61%|██████    | 78/128 [00:01<00:00, 61.82it/s]\u001b[A\n",
      "Epoch 9:  90%|█████████ | 384/426 [00:15<00:01, 25.52it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  92%|█████████▏| 392/426 [00:15<00:01, 25.86it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  94%|█████████▍| 400/426 [00:15<00:00, 26.20it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  96%|█████████▌| 408/426 [00:15<00:00, 26.53it/s, loss=2.93, v_num=]\n",
      "Epoch 9:  98%|█████████▊| 416/426 [00:15<00:00, 26.84it/s, loss=2.93, v_num=]\n",
      "Epoch 9: 100%|██████████| 426/426 [00:15<00:00, 27.14it/s, loss=2.93, v_num=]\n",
      "Epoch 10:  70%|██████▉   | 298/426 [00:13<00:05, 22.19it/s, loss=2.45, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  71%|███████▏  | 304/426 [00:13<00:05, 22.09it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  73%|███████▎  | 312/426 [00:13<00:05, 22.49it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  75%|███████▌  | 320/426 [00:14<00:04, 22.85it/s, loss=2.45, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 48.88it/s]\u001b[A\n",
      "Epoch 10:  77%|███████▋  | 328/426 [00:14<00:04, 23.23it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  79%|███████▉  | 336/426 [00:14<00:03, 23.60it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  81%|████████  | 344/426 [00:14<00:03, 23.96it/s, loss=2.45, v_num=]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:  83%|████████▎ | 352/426 [00:14<00:03, 24.33it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  85%|████████▍ | 360/426 [00:14<00:02, 24.70it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  86%|████████▋ | 368/426 [00:14<00:02, 25.05it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  88%|████████▊ | 376/426 [00:14<00:01, 25.37it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  90%|█████████ | 384/426 [00:14<00:01, 25.72it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  92%|█████████▏| 392/426 [00:15<00:01, 26.05it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  94%|█████████▍| 400/426 [00:15<00:00, 26.37it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  96%|█████████▌| 408/426 [00:15<00:00, 26.67it/s, loss=2.45, v_num=]\n",
      "Epoch 10:  98%|█████████▊| 416/426 [00:15<00:00, 26.99it/s, loss=2.45, v_num=]\n",
      "Epoch 10: 100%|█████████▉| 424/426 [00:15<00:00, 27.31it/s, loss=2.45, v_num=]\n",
      "Epoch 10: 100%|██████████| 426/426 [00:15<00:00, 27.28it/s, loss=2.45, v_num=]\n",
      "Epoch 11:  70%|██████▉   | 298/426 [00:13<00:05, 21.99it/s, loss=1.96, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  71%|███████▏  | 304/426 [00:13<00:05, 21.88it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  73%|███████▎  | 312/426 [00:14<00:05, 22.27it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  75%|███████▌  | 320/426 [00:14<00:04, 22.66it/s, loss=1.96, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 51.35it/s]\u001b[A\n",
      "Epoch 11:  77%|███████▋  | 328/426 [00:14<00:04, 23.03it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  79%|███████▉  | 336/426 [00:14<00:03, 23.40it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  81%|████████  | 344/426 [00:14<00:03, 23.74it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  83%|████████▎ | 352/426 [00:14<00:03, 24.08it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  85%|████████▍ | 360/426 [00:14<00:02, 24.42it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  86%|████████▋ | 368/426 [00:14<00:02, 24.75it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  88%|████████▊ | 376/426 [00:14<00:01, 25.08it/s, loss=1.96, v_num=]\n",
      "Validating:  61%|██████    | 78/128 [00:01<00:00, 63.26it/s]\u001b[A\n",
      "Epoch 11:  90%|█████████ | 384/426 [00:15<00:01, 25.40it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  92%|█████████▏| 392/426 [00:15<00:01, 25.72it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  94%|█████████▍| 400/426 [00:15<00:00, 26.04it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  96%|█████████▌| 408/426 [00:15<00:00, 26.36it/s, loss=1.96, v_num=]\n",
      "Epoch 11:  98%|█████████▊| 416/426 [00:15<00:00, 26.66it/s, loss=1.96, v_num=]\n",
      "Epoch 11: 100%|██████████| 426/426 [00:15<00:00, 26.95it/s, loss=1.96, v_num=]\n",
      "Epoch 12:  70%|██████▉   | 298/426 [00:13<00:05, 21.98it/s, loss=1.58, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  71%|███████▏  | 304/426 [00:13<00:05, 21.88it/s, loss=1.58, v_num=]\n",
      "Validating:   5%|▍         | 6/128 [00:00<00:05, 21.77it/s]\u001b[A\n",
      "Epoch 12:  73%|███████▎  | 312/426 [00:14<00:05, 22.24it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  75%|███████▌  | 320/426 [00:14<00:04, 22.62it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  77%|███████▋  | 328/426 [00:14<00:04, 22.99it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  79%|███████▉  | 336/426 [00:14<00:03, 23.35it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  81%|████████  | 344/426 [00:14<00:03, 23.71it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  83%|████████▎ | 352/426 [00:14<00:03, 24.06it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  85%|████████▍ | 360/426 [00:14<00:02, 24.42it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  86%|████████▋ | 368/426 [00:14<00:02, 24.76it/s, loss=1.58, v_num=]\n",
      "Validating:  55%|█████▍    | 70/128 [00:01<00:00, 65.63it/s]\u001b[A\n",
      "Epoch 12:  88%|████████▊ | 376/426 [00:14<00:01, 25.09it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  90%|█████████ | 384/426 [00:15<00:01, 25.43it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  92%|█████████▏| 392/426 [00:15<00:01, 25.76it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  94%|█████████▍| 400/426 [00:15<00:00, 26.06it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  96%|█████████▌| 408/426 [00:15<00:00, 26.37it/s, loss=1.58, v_num=]\n",
      "Epoch 12:  98%|█████████▊| 416/426 [00:15<00:00, 26.66it/s, loss=1.58, v_num=]\n",
      "Epoch 12: 100%|█████████▉| 424/426 [00:15<00:00, 26.98it/s, loss=1.58, v_num=]\n",
      "Epoch 12: 100%|██████████| 426/426 [00:15<00:00, 26.95it/s, loss=1.58, v_num=]\n",
      "Epoch 13:  70%|██████▉   | 298/426 [00:13<00:05, 21.82it/s, loss=1.26, v_num=] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  71%|███████▏  | 304/426 [00:13<00:05, 21.73it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  73%|███████▎  | 312/426 [00:14<00:05, 22.11it/s, loss=1.26, v_num=]\n",
      "Validating:  11%|█         | 14/128 [00:00<00:02, 39.44it/s]\u001b[A\n",
      "Epoch 13:  75%|███████▌  | 320/426 [00:14<00:04, 22.47it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  77%|███████▋  | 328/426 [00:14<00:04, 22.86it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  79%|███████▉  | 336/426 [00:14<00:03, 23.22it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  81%|████████  | 344/426 [00:14<00:03, 23.58it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  83%|████████▎ | 352/426 [00:14<00:03, 23.94it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  85%|████████▍ | 360/426 [00:14<00:02, 24.29it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  86%|████████▋ | 368/426 [00:14<00:02, 24.63it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  88%|████████▊ | 376/426 [00:15<00:02, 24.97it/s, loss=1.26, v_num=]\n",
      "Validating:  61%|██████    | 78/128 [00:01<00:00, 65.45it/s]\u001b[A\n",
      "Epoch 13:  90%|█████████ | 384/426 [00:15<00:01, 25.28it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  92%|█████████▏| 392/426 [00:15<00:01, 25.57it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  94%|█████████▍| 400/426 [00:15<00:01, 25.90it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  96%|█████████▌| 408/426 [00:15<00:00, 26.22it/s, loss=1.26, v_num=]\n",
      "Epoch 13:  98%|█████████▊| 416/426 [00:15<00:00, 26.54it/s, loss=1.26, v_num=]\n",
      "Epoch 13: 100%|█████████▉| 424/426 [00:15<00:00, 26.85it/s, loss=1.26, v_num=]\n",
      "Epoch 13: 100%|██████████| 426/426 [00:15<00:00, 26.82it/s, loss=1.26, v_num=]\n",
      "Epoch 14:  70%|██████▉   | 298/426 [00:13<00:05, 21.69it/s, loss=1.02, v_num=] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  71%|███████▏  | 304/426 [00:14<00:05, 21.64it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  73%|███████▎  | 312/426 [00:14<00:05, 22.02it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  75%|███████▌  | 320/426 [00:14<00:04, 22.39it/s, loss=1.02, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 49.85it/s]\u001b[A\n",
      "Epoch 14:  77%|███████▋  | 328/426 [00:14<00:04, 22.75it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  79%|███████▉  | 336/426 [00:14<00:03, 23.11it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  81%|████████  | 344/426 [00:14<00:03, 23.47it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  83%|████████▎ | 352/426 [00:14<00:03, 23.81it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  85%|████████▍ | 360/426 [00:14<00:02, 24.14it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  86%|████████▋ | 368/426 [00:15<00:02, 24.48it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  88%|████████▊ | 376/426 [00:15<00:02, 24.82it/s, loss=1.02, v_num=]\n",
      "Validating:  61%|██████    | 78/128 [00:01<00:00, 64.56it/s]\u001b[A\n",
      "Epoch 14:  90%|█████████ | 384/426 [00:15<00:01, 25.15it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  92%|█████████▏| 392/426 [00:15<00:01, 25.45it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  94%|█████████▍| 400/426 [00:15<00:01, 25.77it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  96%|█████████▌| 408/426 [00:15<00:00, 26.08it/s, loss=1.02, v_num=]\n",
      "Epoch 14:  98%|█████████▊| 416/426 [00:15<00:00, 26.40it/s, loss=1.02, v_num=]\n",
      "Epoch 14: 100%|██████████| 426/426 [00:15<00:00, 26.70it/s, loss=1.02, v_num=]\n",
      "Epoch 15:  70%|██████▉   | 298/426 [00:13<00:05, 21.62it/s, loss=0.822, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  71%|███████▏  | 304/426 [00:14<00:05, 21.57it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  73%|███████▎  | 312/426 [00:14<00:05, 21.95it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  75%|███████▌  | 320/426 [00:14<00:04, 22.31it/s, loss=0.822, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 49.56it/s]\u001b[A\n",
      "Epoch 15:  77%|███████▋  | 328/426 [00:14<00:04, 22.67it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  79%|███████▉  | 336/426 [00:14<00:03, 23.02it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  81%|████████  | 344/426 [00:14<00:03, 23.38it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  83%|████████▎ | 352/426 [00:14<00:03, 23.72it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  85%|████████▍ | 360/426 [00:14<00:02, 24.06it/s, loss=0.822, v_num=]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  86%|████████▋ | 368/426 [00:15<00:02, 24.38it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  88%|████████▊ | 376/426 [00:15<00:02, 24.72it/s, loss=0.822, v_num=]\n",
      "Validating:  61%|██████    | 78/128 [00:01<00:00, 63.55it/s]\u001b[A\n",
      "Epoch 15:  90%|█████████ | 384/426 [00:15<00:01, 25.02it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  92%|█████████▏| 392/426 [00:15<00:01, 25.33it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  94%|█████████▍| 400/426 [00:15<00:01, 25.65it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  96%|█████████▌| 408/426 [00:15<00:00, 25.97it/s, loss=0.822, v_num=]\n",
      "Epoch 15:  98%|█████████▊| 416/426 [00:15<00:00, 26.26it/s, loss=0.822, v_num=]\n",
      "Epoch 15: 100%|█████████▉| 424/426 [00:15<00:00, 26.58it/s, loss=0.822, v_num=]\n",
      "Epoch 15: 100%|██████████| 426/426 [00:16<00:00, 26.55it/s, loss=0.822, v_num=]\n",
      "Epoch 16:  70%|██████▉   | 298/426 [00:13<00:05, 21.38it/s, loss=0.691, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  71%|███████▏  | 304/426 [00:14<00:05, 21.32it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  73%|███████▎  | 312/426 [00:14<00:05, 21.70it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  75%|███████▌  | 320/426 [00:14<00:04, 22.06it/s, loss=0.691, v_num=]\n",
      "Validating:  17%|█▋        | 22/128 [00:00<00:02, 48.54it/s]\u001b[A\n",
      "Epoch 16:  77%|███████▋  | 328/426 [00:14<00:04, 22.40it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  79%|███████▉  | 336/426 [00:14<00:03, 22.74it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  81%|████████  | 344/426 [00:14<00:03, 23.11it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  83%|████████▎ | 352/426 [00:15<00:03, 23.44it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  85%|████████▍ | 360/426 [00:15<00:02, 23.78it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  86%|████████▋ | 368/426 [00:15<00:02, 24.10it/s, loss=0.691, v_num=]\n",
      "Validating:  55%|█████▍    | 70/128 [00:01<00:00, 61.72it/s]\u001b[A\n",
      "Epoch 16:  88%|████████▊ | 376/426 [00:15<00:02, 24.41it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  90%|█████████ | 384/426 [00:15<00:01, 24.73it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  92%|█████████▏| 392/426 [00:15<00:01, 25.06it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  94%|█████████▍| 400/426 [00:15<00:01, 25.38it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  96%|█████████▌| 408/426 [00:15<00:00, 25.69it/s, loss=0.691, v_num=]\n",
      "Epoch 16:  98%|█████████▊| 416/426 [00:16<00:00, 25.98it/s, loss=0.691, v_num=]\n",
      "Epoch 16: 100%|█████████▉| 424/426 [00:16<00:00, 26.29it/s, loss=0.691, v_num=]\n",
      "Epoch 16: 100%|██████████| 426/426 [00:16<00:00, 26.27it/s, loss=0.691, v_num=]\n",
      "Epoch 17:  70%|██████▉   | 298/426 [00:13<00:05, 21.98it/s, loss=0.643, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  71%|███████▏  | 304/426 [00:13<00:05, 21.87it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  73%|███████▎  | 312/426 [00:14<00:05, 22.26it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  75%|███████▌  | 320/426 [00:14<00:04, 22.65it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  77%|███████▋  | 328/426 [00:14<00:04, 23.03it/s, loss=0.643, v_num=]\n",
      "Validating:  23%|██▎       | 30/128 [00:00<00:01, 57.77it/s]\u001b[A\n",
      "Epoch 17:  79%|███████▉  | 336/426 [00:14<00:03, 23.40it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  81%|████████  | 344/426 [00:14<00:03, 23.76it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  83%|████████▎ | 352/426 [00:14<00:03, 24.11it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  85%|████████▍ | 360/426 [00:14<00:02, 24.44it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  86%|████████▋ | 368/426 [00:14<00:02, 24.78it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  88%|████████▊ | 376/426 [00:14<00:01, 25.13it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  90%|█████████ | 384/426 [00:15<00:01, 25.47it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  92%|█████████▏| 392/426 [00:15<00:01, 25.81it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  94%|█████████▍| 400/426 [00:15<00:00, 26.13it/s, loss=0.643, v_num=]\n",
      "Validating:  80%|███████▉  | 102/128 [00:01<00:00, 68.08it/s]\u001b[A\n",
      "Epoch 17:  96%|█████████▌| 408/426 [00:15<00:00, 26.45it/s, loss=0.643, v_num=]\n",
      "Epoch 17:  98%|█████████▊| 416/426 [00:15<00:00, 26.76it/s, loss=0.643, v_num=]\n",
      "Epoch 17: 100%|██████████| 426/426 [00:15<00:00, 27.04it/s, loss=0.643, v_num=]\n",
      "Epoch 18:  41%|████      | 174/426 [00:07<00:11, 22.10it/s, loss=0.425, v_num=]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import wandb\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath('.'))\n",
    "load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, '.env'))\n",
    "\n",
    "# import pdb\n",
    "\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser.add_argument('-wandb_run_name',\n",
    "                '--wandb_run_name',\n",
    "                help='Name of Wandb Run',\n",
    "                default='run',\n",
    "                type=str)\n",
    "parser.add_argument('-wandb_project_name',\n",
    "                    '--wandb_project_name',\n",
    "                    help='Wandb Project Name',\n",
    "                    default='deep_dream',\n",
    "                    type=str)\n",
    "parser.add_argument('-model_ckpt_path',\n",
    "                    '--model_ckpt_path',\n",
    "                    help='Model Checkpoint Path',\n",
    "                    default='./ckpts/model.ckpt',\n",
    "                    type=str)\n",
    "parser.add_argument('-wandb_log_num_iter',\n",
    "                    '--wandb_log_num_iter',\n",
    "                    help='After how many batches, we will log in training loop',\n",
    "                    default=1,\n",
    "                    type=int)\n",
    "parser.add_argument('-init_ckpt',\n",
    "                    '--init_ckpt',\n",
    "                    help='Initial Ckpt',\n",
    "                    default=None,\n",
    "                    type=str)\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function that will perform all the training\"\"\"\n",
    "    # init module\n",
    "    model = BeamClassifier(args)\n",
    "\n",
    "    # Using Wandblogger so that we can log our results to wandb\n",
    "    wandb.init(name=args.wandb_run_name,\n",
    "               project=args.wandb_project_name,\n",
    "               config=vars(args))\n",
    "        \n",
    "    wandb.watch(model)\n",
    "\n",
    "    # most basic trainer, uses good defaults\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='./ckpts',\n",
    "        filename='{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "    trainer = Trainer(logger=[], \n",
    "                      gpus=args.gpus, \n",
    "                      max_epochs=args.max_nb_epochs, \n",
    "                      resume_from_checkpoint=args.init_ckpt)\n",
    "#     pdb.set_trace()\n",
    "    trainer.fit(model)\n",
    "    trainer.test(model)\n",
    "    \n",
    "    ckpt_path = os.path.join('./ckpt', f\"{args.wandb_project_name}\", f\"{args.wandb_run_name}.ckpt\")\n",
    "    ckpt_base_path = os.path.dirname(ckpt_path)\n",
    "    trainer.save_checkpoint(ckpt_path)\n",
    "    wandb.save(ckpt_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # auto add args from trainer\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # give the module a chance to add own params\n",
    "    # good practice to define LightningModule speficic params in the module\n",
    "    parser = BeamClassifier.add_model_specific_args(parser)\n",
    "\n",
    "    # parse params\n",
    "    args= parser.parse_args(args_str)\n",
    "\n",
    "    seed_everything(123)\n",
    "\n",
    "    model = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "619",
   "language": "python",
   "name": "619"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "cbfaa0c31130d6fe797b1b315fd7af716006f03c19477746c82c11dfd72f132a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
