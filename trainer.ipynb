{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b795342a-7517-4886-b9e7-0cd7dff4ea44",
   "metadata": {},
   "source": [
    "Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1c65832-d841-44a7-8a1f-884627b5e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.10.26)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (1.0.0)\n",
      "Requirement already satisfied: Click>=7.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (3.1.14)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (3.15.8)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: pytorch-lightning in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (1.2.8)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: torchmetrics>=0.2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.4 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (4.60.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (2.4.1)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (1.20.2)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pytorch-lightning) (0.9.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (52.0.0.post20210125)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.37.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.15.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.28.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (4.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (1.6.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Requirement already satisfied: segmentation-models-pytorch in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.1.3)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.7.4)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.6.3)\n",
      "Requirement already satisfied: timm==0.3.2 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.3.2)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from segmentation-models-pytorch) (0.9.1)\n",
      "Requirement already satisfied: torch in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.8.1)\n",
      "Requirement already satisfied: munch in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (2.5.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.60.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch) (1.20.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (8.2.0)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n",
      "Requirement already satisfied: albumentations in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.4.6)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (1.6.2)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (1.20.2)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (3.4.1)\n",
      "Requirement already satisfied: imageio in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (8.2.0)\n",
      "Requirement already satisfied: Shapely in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from imgaug>=0.4.0->albumentations) (0.18.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (2021.4.8)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (4.4.2)\n",
      "Requirement already satisfied: python-dotenv in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.17.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Requirement already satisfied: torchmetrics in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torchmetrics) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (1.20.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb --upgrade\n",
    "!pip install pytorch-lightning\n",
    "!pip install segmentation-models-pytorch\n",
    "!pip install albumentations\n",
    "!pip install python-dotenv\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6948814-dd3b-4e5e-afc6-8c5fd333c5a5",
   "metadata": {},
   "source": [
    "Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770390a6-37ae-4dce-af77-a0f2f4a2bf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-17 10:16:42--  https://docs.google.com/uc?export=download&confirm=l9uL&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5\n",
      "Resolving docs.google.com (docs.google.com)... 142.250.183.46, 2404:6800:4009:829::200e\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.183.46|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0o-00-docs.googleusercontent.com/docs/securesc/qrb3enrkla6r4av3dvm7pu497tqobdbc/b67tlt40gn75il6i8gd339uif4k9i4ke/1618654575000/18183255587859120126/08827697122404871349Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download [following]\n",
      "--2021-04-17 10:16:42--  https://doc-0o-00-docs.googleusercontent.com/docs/securesc/qrb3enrkla6r4av3dvm7pu497tqobdbc/b67tlt40gn75il6i8gd339uif4k9i4ke/1618654575000/18183255587859120126/08827697122404871349Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download\n",
      "Resolving doc-0o-00-docs.googleusercontent.com (doc-0o-00-docs.googleusercontent.com)... 142.250.67.225, 2404:6800:4009:814::2001\n",
      "Connecting to doc-0o-00-docs.googleusercontent.com (doc-0o-00-docs.googleusercontent.com)|142.250.67.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://docs.google.com/nonceSigner?nonce=nrb4ergf4s6bi&continue=https://doc-0o-00-docs.googleusercontent.com/docs/securesc/qrb3enrkla6r4av3dvm7pu497tqobdbc/b67tlt40gn75il6i8gd339uif4k9i4ke/1618654575000/18183255587859120126/08827697122404871349Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e%3Ddownload&hash=amtue1jretnf56vkbl33vuj9hhivp5c7 [following]\n",
      "--2021-04-17 10:16:43--  https://docs.google.com/nonceSigner?nonce=nrb4ergf4s6bi&continue=https://doc-0o-00-docs.googleusercontent.com/docs/securesc/qrb3enrkla6r4av3dvm7pu497tqobdbc/b67tlt40gn75il6i8gd339uif4k9i4ke/1618654575000/18183255587859120126/08827697122404871349Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e%3Ddownload&hash=amtue1jretnf56vkbl33vuj9hhivp5c7\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.183.46|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://doc-0o-00-docs.googleusercontent.com/docs/securesc/qrb3enrkla6r4av3dvm7pu497tqobdbc/b67tlt40gn75il6i8gd339uif4k9i4ke/1618654575000/18183255587859120126/08827697122404871349Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download&nonce=nrb4ergf4s6bi&user=08827697122404871349Z&hash=ifrmlmdmhk6f5spa9dumg5rvtr7d9ch7 [following]\n",
      "--2021-04-17 10:16:43--  https://doc-0o-00-docs.googleusercontent.com/docs/securesc/qrb3enrkla6r4av3dvm7pu497tqobdbc/b67tlt40gn75il6i8gd339uif4k9i4ke/1618654575000/18183255587859120126/08827697122404871349Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download&nonce=nrb4ergf4s6bi&user=08827697122404871349Z&hash=ifrmlmdmhk6f5spa9dumg5rvtr7d9ch7\n",
      "Connecting to doc-0o-00-docs.googleusercontent.com (doc-0o-00-docs.googleusercontent.com)|142.250.67.225|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘download.tar.gz’\n",
      "\n",
      "download.tar.gz         [         <=>        ]  42.85M  19.9MB/s               "
     ]
    }
   ],
   "source": [
    "# https://drive.google.com/file/d//view?usp=sharing\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5\" -O \"download.tar.gz\" && rm -rf /tmp/cookies.txt\n",
    "!tar -xvf download.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0efbb-756a-4fb1-bc17-a35f607100fd",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a1b223-8e29-4ca4-af92-1914955fab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pdb\n",
    "\n",
    "def calc_acc(pred: torch.tensor, y: torch.tensor, num_classes: int, return_class_wise_acc: bool = False):\n",
    "    pred = pred.argmax(1)\n",
    "    class_wise_acc = []\n",
    "    for i in range(num_classes):\n",
    "        tp = ((pred == i) & (y == i)).sum().float()\n",
    "        tn = ((pred != i) & (y != i)).sum().float()\n",
    "        fp = ((pred == i) & (y != i)).sum().float()\n",
    "        fn = ((pred != i) & (y == i)).sum().float()\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "        class_wise_acc.append(acc)\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "    class_wise_acc = torch.Tensor(class_wise_acc)\n",
    "    if return_class_wise_acc:\n",
    "        return class_wise_acc\n",
    "    return class_wise_acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c6ff2-7976-4fbb-89f4-30fb183a4c39",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9721225a-1517-4b2a-ab84-a8e87035965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b440c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamPredictionDataset(Dataset):\n",
    "    \"\"\"Beam Prediction dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str,\n",
    "                 label_file_path: str,\n",
    "                 reshape: bool=False,\n",
    "                 transforms: Optional[transforms.Compose] = None,\n",
    "                 preprocessing_fn: Optional[transforms.Compose] = None) -> None:\n",
    "        \"\"\"\n",
    "        Init the Dataset\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.label_file_path = label_file_path\n",
    "        \n",
    "        self.data = np.load(file_path)\n",
    "        self.data = self.data.transpose((1, 0))\n",
    "        \n",
    "        self.label = np.load(label_file_path)\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        assert len(self.label) == len(self.data)\n",
    "        # reshape is true\n",
    "        # num_users x num_channels x num_antennas x real/imaginary\n",
    "        if reshape:\n",
    "            self.data = self.data.reshape((2, 4, 32, -1))\n",
    "            self.data = self.data.transpose((3, 2, 1, 0))\n",
    "            \n",
    "        self.preprocessing_fn = preprocessing_fn\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Gets an item from dataset\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        user_data = self.data[idx]\n",
    "        \n",
    "        if self.preprocessing_fn is not None:\n",
    "            user_data = self.preprocessing_fn(user_data)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            user_data = self.transforms(user_data)\n",
    "        \n",
    "        label = self.label[idx]\n",
    "        label = torch.Tensor(label).type(torch.int64) - 1\n",
    "        return user_data, label\n",
    "    \n",
    "def transform(x: np.array) -> torch.Tensor:\n",
    "    # mean normalize\n",
    "    x -= x.mean()\n",
    "    x /= x.std()\n",
    "    x = torch.Tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d379ca-5d25-4285-b0ff-701e75893bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 1.1344,  1.1642,  0.8118,  0.1093, -0.2929,  0.3619,  1.0790,  1.3874,\n",
      "         1.2894,  1.1469,  1.3742,  0.6998, -0.4009,  0.3625,  0.8699,  1.1849,\n",
      "         0.6786,  1.1753,  1.1628,  1.0550, -0.8943, -0.7125,  0.3022,  1.0900,\n",
      "         0.8706,  1.3478,  1.5702,  0.9558, -1.6566, -0.7382,  0.0148,  0.6442,\n",
      "         0.3017,  0.8905,  0.9791,  1.1644, -0.9876, -0.7922, -0.4629,  0.3374,\n",
      "        -0.0516,  0.6355,  1.2137,  1.3104, -1.2456, -1.1153, -1.0029,  0.0712,\n",
      "        -0.3255,  0.3647,  1.0288,  1.1176, -1.1510, -1.2242, -1.2269, -0.4542,\n",
      "        -0.7970, -0.4256,  0.9413,  1.4918, -0.9748, -1.5398, -1.6161, -0.4450,\n",
      "        -1.1006, -0.9013,  0.4272,  1.0205, -0.8424, -1.2304, -1.5746, -1.4980,\n",
      "        -1.5050, -0.9478, -0.2943,  0.4626, -0.4176, -1.3170, -1.5857, -1.3130,\n",
      "        -1.2471, -1.1989, -0.6621,  0.2399, -0.2827, -1.0135, -1.6343, -1.5665,\n",
      "        -1.4390, -1.5518, -0.7836, -0.4450, -0.0606, -0.6873, -1.3370, -2.0944,\n",
      "        -1.3866, -1.2450, -1.3194, -0.6425,  0.7051, -0.0322, -1.0300, -1.2306,\n",
      "        -0.6418, -1.6214, -1.3991, -0.9170,  0.9509,  0.4661, -0.6956, -0.8484,\n",
      "        -0.9170, -1.3425, -1.5632, -1.4795,  1.4220,  0.5489,  0.4026, -0.5844,\n",
      "        -0.6274, -1.2085, -1.7199, -1.2470,  1.5720,  1.2481,  0.3709,  0.0025,\n",
      "         0.3596, -0.8391, -1.3560, -1.4393,  1.5227,  1.0183,  0.8746, -0.1959,\n",
      "         0.3806, -0.0644, -1.1502, -1.6148,  1.1727,  1.3091,  1.0675,  0.4314,\n",
      "         1.0033,  0.0746, -0.6809, -1.2830,  1.1760,  1.6166,  1.5842,  0.6260,\n",
      "         1.1363,  0.2335, -0.0576, -1.0852,  0.6769,  1.3474,  1.4323,  1.1049,\n",
      "         1.5561,  0.5584,  0.0851, -0.4431,  0.5574,  0.8314,  1.7251,  0.9947,\n",
      "         1.3336,  1.2545,  0.2566, -0.4365, -0.1540,  0.6379,  1.1497,  1.0744,\n",
      "         1.1799,  1.3945,  0.7851,  0.2137, -0.2622,  0.1544,  1.4368,  1.8656,\n",
      "         1.3089,  1.3896,  1.0659,  0.2861, -0.3666,  0.1218,  0.7796,  1.2413,\n",
      "         1.2102,  1.3221,  1.0344,  0.9304, -1.0877, -0.1499,  0.3859,  1.0914,\n",
      "         0.4617,  0.7131,  1.6144,  1.1880, -1.0281, -1.3422,  0.1930,  0.9728,\n",
      "         0.1454,  0.8132,  1.3767,  0.8216, -1.3210, -0.6237, -0.2125, -0.0454,\n",
      "        -0.3330,  0.3443,  0.7524,  0.9035, -1.2856, -0.2620, -0.4970, -0.2679,\n",
      "        -0.8429,  0.5761,  0.9023,  1.2007, -1.4464, -1.1691, -0.6407, -0.2095,\n",
      "        -0.7193,  0.1202,  0.4905,  1.4410, -1.2580, -1.2274, -1.1168, -0.4818,\n",
      "        -0.5149, -0.4690,  0.3170,  0.8014, -0.6843, -1.1411, -0.9833, -0.9430,\n",
      "        -0.6822, -0.7225, -0.5029,  0.7130, -0.4280, -0.8024, -1.1280, -0.9917]), tensor([36]))\n"
     ]
    }
   ],
   "source": [
    "# example ds\n",
    "\n",
    "ds = BeamPredictionDataset(\n",
    "    file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
    "    label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
    "    transforms=transform\n",
    ")\n",
    "it = iter(ds)\n",
    "out = next(it)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569287fc-a66a-487b-9de4-9918e86979dc",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e84e46-5c5f-4886-80ab-8163ebd91780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from argparse import ArgumentParser\n",
    "import segmentation_models_pytorch as smp\n",
    "from typing import Tuple\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "\n",
    "import torchmetrics \n",
    "import pdb\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class BeamClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams) -> None:\n",
    "        \"\"\"\n",
    "        Downloading Backbone and defining structure of model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # args from argparser\n",
    "        self.hparams = hparams\n",
    "        in_ch = self.hparams.in_ch\n",
    "        out_ch = self.hparams.out_ch\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_ch, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(2048, out_ch),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward step of model.\n",
    "        \"\"\"\n",
    "#         pdb.set_trace()\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def loss_fn(self, pred: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Loss function used for model\"\"\"\n",
    "        y_sq = y.squeeze(-1)\n",
    "        loss = F.cross_entropy(pred, y_sq)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim:\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        opt = torch.optim.Adam(self.model.parameters(),\n",
    "                               lr=self.hparams.learning_rate)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            opt, self.hparams.max_nb_epochs, self.hparams.learning_rate)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "#         lr_scheduler = None\n",
    "        return [opt], [lr_scheduler]\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the data loader for training data\"\"\"\n",
    "        # REQUIRED\n",
    "        return DataLoader(BeamPredictionDataset(\n",
    "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
    "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
    "                                transforms=transform\n",
    "                            ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "    def training_step(self, batch: list, batch_idx: int) -> dict:\n",
    "        \"\"\"Backward step of model\"\"\"\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        pred = F.softmax(pred, dim=-1)\n",
    "        y_sq = y.squeeze(-1)\n",
    "        \n",
    "        # metrics\n",
    "        acc_metric = Accuracy()\n",
    "        acc = acc_metric(pred, y_sq)\n",
    "        precision = Precision(average='macro', num_classes=self.hparams.out_ch)\n",
    "        prec = precision(pred, y_sq)\n",
    "        recall = Recall(average='macro', num_classes=self.hparams.out_ch)\n",
    "        rec = recall(pred, y_sq)\n",
    "        \n",
    "        if self.lr_scheduler is not None:\n",
    "            lr = self.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        if(batch_idx % self.hparams.wandb_log_num_iter == 0):\n",
    "            wandb.log({\n",
    "                'train_loss': loss,\n",
    "            })\n",
    "            \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'train_acc': acc,\n",
    "            'train_prec': prec,\n",
    "            'train_rec': rec\n",
    "        }\n",
    "    \n",
    "    def training_epoch_end(self, outputs: list) -> None:\n",
    "        acc = torch.stack([x['train_acc'] for x in outputs]).mean()\n",
    "        prec = torch.stack([x['train_prec'] for x in outputs]).mean()\n",
    "        rec = torch.stack([x['train_rec'] for x in outputs]).mean()\n",
    "        \n",
    "        if self.lr_scheduler is not None:\n",
    "            lr = self.lr_scheduler.get_last_lr()[0]\n",
    "        else:\n",
    "            lr = self.hparams.learning_rate\n",
    "        logs = {\n",
    "            'lr': lr,\n",
    "            'train_acc': acc,\n",
    "            'train_prec': prec,\n",
    "            'train_rec': rec\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "        self.log_dict(logs)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the data loader for validation data\"\"\"\n",
    "        # OPTIONAL\n",
    "        return DataLoader(BeamPredictionDataset(\n",
    "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpVal.npy',\n",
    "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelVal.npy',\n",
    "                                transforms=transform\n",
    "                            ),\n",
    "                          batch_size=self.hparams.batch_size)\n",
    "\n",
    "    def validation_step(self, batch: list, batch_idx: torch.Tensor) -> dict:\n",
    "        \"\"\"Validation step to be carried out on validation data.\"\"\"\n",
    "        # REQUIRED\n",
    "        # pdb.set_trace()\n",
    "        x, y = batch\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        pred = F.softmax(pred, dim=-1)\n",
    "        y_sq = y.squeeze(-1)\n",
    "        acc_metric = torchmetrics.Accuracy()\n",
    "        acc = acc_metric(pred, y_sq)\n",
    "        F1_metric = torchmetrics.F1(num_classes=self.hparams.out_ch)\n",
    "        f1 = F1_metric(pred, y_sq)\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': acc,\n",
    "            'val_f1': prec,\n",
    "            'val_rec': rec\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list) -> None:\n",
    "        \"\"\"Use results from each validation step to generate validation stats at epoch end\"\"\"\n",
    "        val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        val_prec = torch.stack([x['val_prec'] for x in outputs]).mean()\n",
    "        val_rec = torch.stack([x['val_rec'] for x in outputs]).mean()\n",
    "        \n",
    "        logs = {\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_prec': val_prec,\n",
    "            'val_rec': val_rec\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "        self.log_dict(logs)\n",
    "\n",
    "    def load_encoder_weights(self) -> None:\n",
    "        \"\"\"Loads encoder weights from ckpt\"\"\"\n",
    "        ckpt = torch.load(self.hparams.encoder_ckpt_path)\n",
    "        pretrained_dict = ckpt['state_dict']\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if (\n",
    "            'encoder' in k) and (k in model_dict)}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def load_model_weights_from_ckpt(self) -> None:\n",
    "        \"\"\"Load model weights to model on cpu\"\"\"\n",
    "        ckpt = torch.load(self.hparams.model_ckpt_path,\n",
    "                          map_location=torch.device('cpu'))\n",
    "        pretrained_dict = ckpt['state_dict']\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in pretrained_dict.items() if (k in model_dict)}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        \n",
    "    def _get_learning_rate(self) -> float:\n",
    "        i = 0\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            if i == 0:\n",
    "                learning_rate = param_group[\"lr\"]\n",
    "            else:\n",
    "                if learning_rate != param_group[\"lr\"]:\n",
    "                    raise ValueError(\n",
    "                        \"different param groups have different lr\")\n",
    "        return learning_rate\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n",
    "        \"\"\"\n",
    "        Specify the hyperparams for this LightningModule\n",
    "        \"\"\"\n",
    "        # MODEL specific arguments\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--learning_rate', default=0.02, type=float)\n",
    "        parser.add_argument('--batch_size', default=32, type=int)\n",
    "        parser.add_argument('--in_ch', default=256, type=int)\n",
    "        parser.add_argument('--out_ch', default=64, type=int)\n",
    "        parser.add_argument('--max_nb_epochs', default=1, type=int)\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089fff89-0a41-4ee7-9954-7feb94bd577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2097, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser, Namespace\n",
    "args_str = [\n",
    "        # model related args\n",
    "        '--max_nb_epochs=1',\n",
    "        '--learning_rate=1e-3',\n",
    "        '--batch_size=16',\n",
    "        '--in_ch=256',\n",
    "        '--out_ch=64',\n",
    "]\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser = BeamClassifier.add_model_specific_args(parser)\n",
    "args= parser.parse_args(args_str)\n",
    "\n",
    "model = BeamClassifier(args)\n",
    "train_dl = model.train_dataloader()\n",
    "it = iter(train_dl)\n",
    "x, y = next(it)\n",
    "\n",
    "\n",
    "pred = model(x)\n",
    "loss = model.loss_fn(pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3697609-9464-472d-8433-2a7e0b8787c7",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "308a8cd8-0086-46c0-972d-104f787c18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_str = ['--tpu_cores=0',\n",
    "        '--progress_bar_refresh_rate=20',\n",
    "        '--wandb_run_name=baseline',\n",
    "        '--wandb_project_name=Beam Prediction',\n",
    "        '--wandb_log_num_iter=1',\n",
    "        '--gpus=0',\n",
    "        # model related args\n",
    "        '--max_nb_epochs=100',\n",
    "        '--learning_rate=5e-3',\n",
    "        '--batch_size=128',\n",
    "        '--in_ch=256',\n",
    "        '--out_ch=64',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3bf4ac2e-b316-459a-a66c-d188569b2237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath('.'))\n",
    "load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07de7ad7-ad9a-4ded-8c30-a1b433c91b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3mj92nw2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 102342<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ubuntu/619/wandb/run-20210417_102446-3mj92nw2/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ubuntu/619/wandb/run-20210417_102446-3mj92nw2/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">baseline</strong>: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/3mj92nw2\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/3mj92nw2</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3mj92nw2). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">baseline</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/2sgn46ni\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/2sgn46ni</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/619/wandb/run-20210417_104245-2sgn46ni</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 17.5 M\n",
      "-------------------------------------\n",
      "17.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "17.5 M    Total params\n",
      "69.853    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/850 [00:00<?, ?it/s]              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  70%|███████   | 595/850 [02:00<00:51,  4.93it/s, loss=1.61, v_num=]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-39f736c54d16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-39f736c54d16>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     50\u001b[0m                       resume_from_checkpoint=args.init_ckpt)\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#     pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./ckpt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{args.wandb_project_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{args.wandb_run_name}.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# log epoch metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         self.trainer.logger_connector.log_train_epoch_end_metrics(\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0mepoch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_accumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping_accumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\u001b[0m in \u001b[0;36mlog_train_epoch_end_metrics\u001b[0;34m(self, epoch_output, checkpoint_accumulator, early_stopping_accumulator, num_optimizers)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_1_0_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;31m# lightning module hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_optimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;31m# log/aggregate metrics automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/619/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py\u001b[0m in \u001b[0;36mtraining_epoch_end\u001b[0;34m(self, model, epoch_output, num_optimizers)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;31m# lightningmodule hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mepoch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-09f20213360e>\u001b[0m in \u001b[0;36mtraining_epoch_end\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         logs = {\n\u001b[1;32m    133\u001b[0m             \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;34m'train_f1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import wandb\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# import pdb\n",
    "\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser.add_argument('-wandb_run_name',\n",
    "                '--wandb_run_name',\n",
    "                help='Name of Wandb Run',\n",
    "                default='run',\n",
    "                type=str)\n",
    "parser.add_argument('-wandb_project_name',\n",
    "                    '--wandb_project_name',\n",
    "                    help='Wandb Project Name',\n",
    "                    default='deep_dream',\n",
    "                    type=str)\n",
    "parser.add_argument('-model_ckpt_path',\n",
    "                    '--model_ckpt_path',\n",
    "                    help='Model Checkpoint Path',\n",
    "                    default='./ckpts/model.ckpt',\n",
    "                    type=str)\n",
    "parser.add_argument('-wandb_log_num_iter',\n",
    "                    '--wandb_log_num_iter',\n",
    "                    help='After how many batches, we will log in training loop',\n",
    "                    default=1,\n",
    "                    type=int)\n",
    "parser.add_argument('-init_ckpt',\n",
    "                    '--init_ckpt',\n",
    "                    help='Initial Ckpt',\n",
    "                    default=None,\n",
    "                    type=str)\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function that will perform all the training\"\"\"\n",
    "    # init module\n",
    "    model = BeamClassifier(args)\n",
    "\n",
    "    # Using Wandblogger so that we can log our results to wandb\n",
    "    wandb.init(name=args.wandb_run_name,\n",
    "               project=args.wandb_project_name,\n",
    "               config=vars(args))\n",
    "        \n",
    "    wandb.watch(model)\n",
    "\n",
    "    # most basic trainer, uses good defaults\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='./ckpts',\n",
    "        filename='{epoch:02d}-{val_loss:.2f}'\n",
    "    )\n",
    "    trainer = Trainer(logger=[], \n",
    "                      gpus=args.gpus, \n",
    "                      max_epochs=args.max_nb_epochs, \n",
    "                      resume_from_checkpoint=args.init_ckpt)\n",
    "#     pdb.set_trace()\n",
    "    trainer.fit(model)\n",
    "    \n",
    "    ckpt_path = os.path.join('./ckpt', f\"{args.wandb_project_name}\", f\"{args.wandb_run_name}.ckpt\")\n",
    "    ckpt_base_path = os.path.dirname(ckpt_path)\n",
    "    trainer.save_checkpoint(ckpt_path)\n",
    "    wandb.save(ckpt_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # auto add args from trainer\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # give the module a chance to add own params\n",
    "    # good practice to define LightningModule speficic params in the module\n",
    "    parser = BeamClassifier.add_model_specific_args(parser)\n",
    "\n",
    "    # parse params\n",
    "    args= parser.parse_args(args_str)\n",
    "\n",
    "    seed_everything(123)\n",
    "\n",
    "    model = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a6b02-2fe2-4955-9f9a-694bdd167c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (619)",
   "language": "python",
   "name": "619"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
