{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b795342a-7517-4886-b9e7-0cd7dff4ea44",
   "metadata": {},
   "source": [
    "Install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1c65832-d841-44a7-8a1f-884627b5e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch>=1.3.1 in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torchmetrics) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (1.20.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb --upgrade\n",
    "!pip install pytorch-lightning\n",
    "!pip install segmentation-models-pytorch\n",
    "!pip install albumentations\n",
    "!pip install python-dotenv\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6948814-dd3b-4e5e-afc6-8c5fd333c5a5",
   "metadata": {},
   "source": [
    "Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770390a6-37ae-4dce-af77-a0f2f4a2bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://drive.google.com/file/d//view?usp=sharing\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5\" -O \"download.tar.gz\" && rm -rf /tmp/cookies.txt\n",
    "!tar -xvf download.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0efbb-756a-4fb1-bc17-a35f607100fd",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1b223-8e29-4ca4-af92-1914955fab86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "928c6ff2-7976-4fbb-89f4-30fb183a4c39",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9721225a-1517-4b2a-ab84-a8e87035965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensor\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b440c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamPredictionDataset(Dataset):\n",
    "    \"\"\"Beam Prediction dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str,\n",
    "                 label_file_path: str,\n",
    "                 reshape: bool=False,\n",
    "                 transforms: Optional[transforms.Compose] = None,\n",
    "                 preprocessing_fn: Optional[transforms.Compose] = None) -> None:\n",
    "        \"\"\"\n",
    "        Init the Dataset\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.label_file_path = label_file_path\n",
    "        \n",
    "        self.data = np.load(file_path)\n",
    "        self.data = self.data.transpose((1, 0))\n",
    "        \n",
    "        self.label = np.load(label_file_path)\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        assert len(self.label) == len(self.data)\n",
    "        # reshape is true\n",
    "        # num_users x num_channels x num_antennas x real/imaginary\n",
    "        if reshape:\n",
    "            self.data = self.data.reshape((2, 4, 32, -1))\n",
    "            self.data = self.data.transpose((3, 2, 1, 0))\n",
    "            \n",
    "        self.preprocessing_fn = preprocessing_fn\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the total length of dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Gets an item from dataset\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        user_data = self.data[idx]\n",
    "        \n",
    "        if self.preprocessing_fn is not None:\n",
    "            user_data = self.preprocessing_fn(user_data)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            user_data = self.transforms(user_data)\n",
    "        \n",
    "        label = self.label[idx]\n",
    "        label = torch.Tensor(label).type(torch.int64) - 1\n",
    "        return user_data, label\n",
    "    \n",
    "def transform(x: np.array) -> torch.Tensor:\n",
    "    # mean normalize\n",
    "    x -= x.mean()\n",
    "    x /= x.std()\n",
    "    x = torch.Tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d379ca-5d25-4285-b0ff-701e75893bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 1.1344,  1.1642,  0.8118,  0.1093, -0.2929,  0.3619,  1.0790,  1.3874,\n",
      "         1.2894,  1.1469,  1.3742,  0.6998, -0.4009,  0.3625,  0.8699,  1.1849,\n",
      "         0.6786,  1.1753,  1.1628,  1.0550, -0.8943, -0.7125,  0.3022,  1.0900,\n",
      "         0.8706,  1.3478,  1.5702,  0.9558, -1.6566, -0.7382,  0.0148,  0.6442,\n",
      "         0.3017,  0.8905,  0.9791,  1.1644, -0.9876, -0.7922, -0.4629,  0.3374,\n",
      "        -0.0516,  0.6355,  1.2137,  1.3104, -1.2456, -1.1153, -1.0029,  0.0712,\n",
      "        -0.3255,  0.3647,  1.0288,  1.1176, -1.1510, -1.2242, -1.2269, -0.4542,\n",
      "        -0.7970, -0.4256,  0.9413,  1.4918, -0.9748, -1.5398, -1.6161, -0.4450,\n",
      "        -1.1006, -0.9013,  0.4272,  1.0205, -0.8424, -1.2304, -1.5746, -1.4980,\n",
      "        -1.5050, -0.9478, -0.2943,  0.4626, -0.4176, -1.3170, -1.5857, -1.3130,\n",
      "        -1.2471, -1.1989, -0.6621,  0.2399, -0.2827, -1.0135, -1.6343, -1.5665,\n",
      "        -1.4390, -1.5518, -0.7836, -0.4450, -0.0606, -0.6873, -1.3370, -2.0944,\n",
      "        -1.3866, -1.2450, -1.3194, -0.6425,  0.7051, -0.0322, -1.0300, -1.2306,\n",
      "        -0.6418, -1.6214, -1.3991, -0.9170,  0.9509,  0.4661, -0.6956, -0.8484,\n",
      "        -0.9170, -1.3425, -1.5632, -1.4795,  1.4220,  0.5489,  0.4026, -0.5844,\n",
      "        -0.6274, -1.2085, -1.7199, -1.2470,  1.5720,  1.2481,  0.3709,  0.0025,\n",
      "         0.3596, -0.8391, -1.3560, -1.4393,  1.5227,  1.0183,  0.8746, -0.1959,\n",
      "         0.3806, -0.0644, -1.1502, -1.6148,  1.1727,  1.3091,  1.0675,  0.4314,\n",
      "         1.0033,  0.0746, -0.6809, -1.2830,  1.1760,  1.6166,  1.5842,  0.6260,\n",
      "         1.1363,  0.2335, -0.0576, -1.0852,  0.6769,  1.3474,  1.4323,  1.1049,\n",
      "         1.5561,  0.5584,  0.0851, -0.4431,  0.5574,  0.8314,  1.7251,  0.9947,\n",
      "         1.3336,  1.2545,  0.2566, -0.4365, -0.1540,  0.6379,  1.1497,  1.0744,\n",
      "         1.1799,  1.3945,  0.7851,  0.2137, -0.2622,  0.1544,  1.4368,  1.8656,\n",
      "         1.3089,  1.3896,  1.0659,  0.2861, -0.3666,  0.1218,  0.7796,  1.2413,\n",
      "         1.2102,  1.3221,  1.0344,  0.9304, -1.0877, -0.1499,  0.3859,  1.0914,\n",
      "         0.4617,  0.7131,  1.6144,  1.1880, -1.0281, -1.3422,  0.1930,  0.9728,\n",
      "         0.1454,  0.8132,  1.3767,  0.8216, -1.3210, -0.6237, -0.2125, -0.0454,\n",
      "        -0.3330,  0.3443,  0.7524,  0.9035, -1.2856, -0.2620, -0.4970, -0.2679,\n",
      "        -0.8429,  0.5761,  0.9023,  1.2007, -1.4464, -1.1691, -0.6407, -0.2095,\n",
      "        -0.7193,  0.1202,  0.4905,  1.4410, -1.2580, -1.2274, -1.1168, -0.4818,\n",
      "        -0.5149, -0.4690,  0.3170,  0.8014, -0.6843, -1.1411, -0.9833, -0.9430,\n",
      "        -0.6822, -0.7225, -0.5029,  0.7130, -0.4280, -0.8024, -1.1280, -0.9917]), tensor([36]))\n"
     ]
    }
   ],
   "source": [
    "# example ds\n",
    "\n",
    "ds = BeamPredictionDataset(\n",
    "    file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
    "    label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
    "    transforms=transform\n",
    ")\n",
    "it = iter(ds)\n",
    "out = next(it)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569287fc-a66a-487b-9de4-9918e86979dc",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e84e46-5c5f-4886-80ab-8163ebd91780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "import wandb\n",
    "from argparse import ArgumentParser\n",
    "import segmentation_models_pytorch as smp\n",
    "from typing import Tuple\n",
    "\n",
    "import torchmetrics \n",
    "import pdb\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class BeamClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams) -> None:\n",
    "        \"\"\"\n",
    "        Downloading Backbone and defining structure of model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # args from argparser\n",
    "        self.hparams = hparams\n",
    "        in_ch = self.hparams.in_ch\n",
    "        out_ch = self.hparams.out_ch\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_ch, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(512, out_ch),\n",
    "        )\n",
    "        \n",
    "        self.acc_metric = torchmetrics.Accuracy()\n",
    "        self.F1_metric = torchmetrics.F1(num_classes=self.hparams.out_ch)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Forward step of model.\n",
    "        \"\"\"\n",
    "#         pdb.set_trace()\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def loss_fn(self, pred: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Loss function used for model\"\"\"\n",
    "        y_sq = y.squeeze(-1)\n",
    "        loss = F.cross_entropy(pred, y_sq)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self) -> torch.optim:\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "        opt = torch.optim.Adam(self.model.parameters(),\n",
    "                               lr=self.hparams.learning_rate)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            opt, self.hparams.max_nb_epochs, self.hparams.learning_rate)\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "#         lr_scheduler = None\n",
    "        return [opt], [lr_scheduler]\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the data loader for training data\"\"\"\n",
    "        # REQUIRED\n",
    "        return DataLoader(BeamPredictionDataset(\n",
    "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
    "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
    "                                transforms=transform\n",
    "                            ),\n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "    def training_step(self, batch: list, batch_idx: int) -> dict:\n",
    "        \"\"\"Backward step of model\"\"\"\n",
    "        # REQUIRED\n",
    "        x, y = batch\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        pred = F.softmax(pred, dim=-1)\n",
    "        y_sq = y.squeeze(-1)\n",
    "        acc = self.acc_metric(pred, y_sq)\n",
    "        f1 = self.F1_metric(pred, y_sq)\n",
    "        \n",
    "        if self.lr_scheduler is not None:\n",
    "            lr = self.lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        if(batch_idx % self.hparams.wandb_log_num_iter == 0):\n",
    "            wandb.log({\n",
    "                'train_loss': loss,\n",
    "            })\n",
    "            \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'train_acc': acc,\n",
    "            'train_f1': f1\n",
    "        }\n",
    "    \n",
    "    def training_epoch_end(self, outputs: list) -> None:\n",
    "        acc = self.acc_metric.compute()\n",
    "        f1 = self.F1_metric.compute()\n",
    "        \n",
    "        if self.lr_scheduler is not None:\n",
    "            lr = self.lr_scheduler.get_last_lr()[0]\n",
    "        else:\n",
    "            lr = self.hparams.learning_rate\n",
    "        logs = {\n",
    "            'lr': lr,\n",
    "            'train_acc': acc,\n",
    "            'train_f1': f1\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "        self.log_dict(logs)\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Define the data loader for validation data\"\"\"\n",
    "        # OPTIONAL\n",
    "        return DataLoader(BeamPredictionDataset(\n",
    "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpVal.npy',\n",
    "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelVal.npy',\n",
    "                                transforms=transform\n",
    "                            ),\n",
    "                          batch_size=self.hparams.batch_size)\n",
    "\n",
    "    def validation_step(self, batch: list, batch_idx: torch.Tensor) -> dict:\n",
    "        \"\"\"Validation step to be carried out on validation data.\"\"\"\n",
    "        # REQUIRED\n",
    "        # pdb.set_trace()\n",
    "        x, y = batch\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        \n",
    "        pred = F.softmax(pred, dim=-1)\n",
    "        y_sq = y.squeeze(-1)\n",
    "        acc = self.acc_metric(pred, y_sq)\n",
    "        f1 = self.F1_metric(pred, y_sq)\n",
    "\n",
    "        return {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': acc,\n",
    "            'val_f1': f1\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list) -> None:\n",
    "        \"\"\"Use results from each validation step to generate validation stats at epoch end\"\"\"\n",
    "        val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        val_acc = self.acc_metric.compute()\n",
    "        val_f1 = self.F1_metric.compute()\n",
    "        \n",
    "        logs = {\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1\n",
    "        }\n",
    "        wandb.log(logs)\n",
    "        self.log_dict(logs)\n",
    "\n",
    "    def load_encoder_weights(self) -> None:\n",
    "        \"\"\"Loads encoder weights from ckpt\"\"\"\n",
    "        ckpt = torch.load(self.hparams.encoder_ckpt_path)\n",
    "        pretrained_dict = ckpt['state_dict']\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if (\n",
    "            'encoder' in k) and (k in model_dict)}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "\n",
    "    def load_model_weights_from_ckpt(self) -> None:\n",
    "        \"\"\"Load model weights to model on cpu\"\"\"\n",
    "        ckpt = torch.load(self.hparams.model_ckpt_path,\n",
    "                          map_location=torch.device('cpu'))\n",
    "        pretrained_dict = ckpt['state_dict']\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k,\n",
    "                           v in pretrained_dict.items() if (k in model_dict)}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n",
    "        \n",
    "    def _get_learning_rate(self) -> float:\n",
    "        i = 0\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            if i == 0:\n",
    "                learning_rate = param_group[\"lr\"]\n",
    "            else:\n",
    "                if learning_rate != param_group[\"lr\"]:\n",
    "                    raise ValueError(\n",
    "                        \"different param groups have different lr\")\n",
    "        return learning_rate\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n",
    "        \"\"\"\n",
    "        Specify the hyperparams for this LightningModule\n",
    "        \"\"\"\n",
    "        # MODEL specific arguments\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--learning_rate', default=0.02, type=float)\n",
    "        parser.add_argument('--batch_size', default=32, type=int)\n",
    "        parser.add_argument('--in_ch', default=256, type=int)\n",
    "        parser.add_argument('--out_ch', default=64, type=int)\n",
    "        parser.add_argument('--max_nb_epochs', default=1, type=int)\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089fff89-0a41-4ee7-9954-7feb94bd577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0360, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser, Namespace\n",
    "args_str = [\n",
    "        # model related args\n",
    "        '--max_nb_epochs=1',\n",
    "        '--learning_rate=1e-3',\n",
    "        '--batch_size=16',\n",
    "        '--in_ch=256',\n",
    "        '--out_ch=64',\n",
    "]\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser = BeamClassifier.add_model_specific_args(parser)\n",
    "args= parser.parse_args(args_str)\n",
    "\n",
    "model = BeamClassifier(args)\n",
    "train_dl = model.train_dataloader()\n",
    "it = iter(train_dl)\n",
    "x, y = next(it)\n",
    "\n",
    "\n",
    "pred = model(x)\n",
    "loss = model.loss_fn(pred, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3697609-9464-472d-8433-2a7e0b8787c7",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308a8cd8-0086-46c0-972d-104f787c18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_str = ['--tpu_cores=0',\n",
    "        '--progress_bar_refresh_rate=20',\n",
    "        '--wandb_run_name=baseline',\n",
    "        '--wandb_project_name=Beam Prediction',\n",
    "        '--wandb_log_num_iter=1',\n",
    "        '--gpus=0',\n",
    "        # model related args\n",
    "        '--max_nb_epochs=100',\n",
    "        '--learning_rate=5e-3',\n",
    "        '--batch_size=128',\n",
    "        '--in_ch=256',\n",
    "        '--out_ch=64',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bf4ac2e-b316-459a-a66c-d188569b2237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath('.'))\n",
    "load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07de7ad7-ad9a-4ded-8c30-a1b433c91b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 123\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2s35q6av) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 91553<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/ubuntu/619/wandb/run-20210417_093016-2s35q6av/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/ubuntu/619/wandb/run-20210417_093016-2s35q6av/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val_loss</td><td>1.45293</td></tr><tr><td>val_acc</td><td>0.23706</td></tr><tr><td>val_f1</td><td>0.23706</td></tr><tr><td>_runtime</td><td>121</td></tr><tr><td>_timestamp</td><td>1618651940</td></tr><tr><td>_step</td><td>3025</td></tr><tr><td>train_loss</td><td>1.80217</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>train_acc</td><td>0.22442</td></tr><tr><td>train_f1</td><td>0.22442</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>val_loss</td><td>█▃▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▆▇▇█</td></tr><tr><td>val_f1</td><td>▁▄▆▇▇█</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▂▂▁▂▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▇█</td></tr><tr><td>train_f1</td><td>▁▄▅▇█</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">beam_trail_1</strong>: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/2s35q6av\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/2s35q6av</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:2s35q6av). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.26<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">beam_trail_1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/1neu2jya\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/1neu2jya</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/619/wandb/run-20210417_093324-1neu2jya</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | model      | Sequential | 1.2 M \n",
      "1 | acc_metric | Accuracy   | 0     \n",
      "2 | F1_metric  | F1         | 0     \n",
      "------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.881     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  70%|███████   | 595/850 [00:27<00:11, 21.35it/s, loss=1.86, v_num=]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0:  71%|███████   | 600/850 [00:27<00:11, 21.45it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  71%|███████▏  | 606/850 [00:28<00:11, 21.58it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  72%|███████▏  | 612/850 [00:28<00:10, 21.72it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  73%|███████▎  | 618/850 [00:28<00:10, 21.85it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  74%|███████▎  | 625/850 [00:28<00:10, 22.01it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  74%|███████▍  | 632/850 [00:28<00:09, 22.17it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  75%|███████▌  | 639/850 [00:28<00:09, 22.32it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  76%|███████▌  | 646/850 [00:28<00:09, 22.48it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  77%|███████▋  | 653/850 [00:28<00:08, 22.63it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  78%|███████▊  | 660/850 [00:28<00:08, 22.78it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  78%|███████▊  | 667/850 [00:29<00:07, 22.93it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  79%|███████▉  | 674/850 [00:29<00:07, 23.08it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  80%|████████  | 681/850 [00:29<00:07, 23.23it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  81%|████████  | 688/850 [00:29<00:06, 23.38it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  82%|████████▏ | 695/850 [00:29<00:06, 23.51it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  83%|████████▎ | 702/850 [00:29<00:06, 23.66it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  83%|████████▎ | 709/850 [00:29<00:05, 23.80it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  84%|████████▍ | 716/850 [00:29<00:05, 23.94it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  85%|████████▌ | 723/850 [00:30<00:05, 24.08it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  86%|████████▌ | 730/850 [00:30<00:04, 24.22it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  87%|████████▋ | 737/850 [00:30<00:04, 24.36it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  88%|████████▊ | 744/850 [00:30<00:04, 24.50it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  88%|████████▊ | 751/850 [00:30<00:04, 24.64it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  89%|████████▉ | 758/850 [00:30<00:03, 24.77it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  90%|█████████ | 765/850 [00:30<00:03, 24.91it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  91%|█████████ | 772/850 [00:30<00:03, 25.05it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  92%|█████████▏| 780/850 [00:30<00:02, 25.23it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  93%|█████████▎| 788/850 [00:31<00:02, 25.39it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  94%|█████████▎| 796/850 [00:31<00:02, 25.53it/s, loss=1.86, v_num=]\n",
      "Validating:  79%|███████▉  | 201/255 [00:03<00:00, 63.77it/s]\u001b[A\n",
      "Epoch 0:  95%|█████████▍| 804/850 [00:31<00:01, 25.68it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  96%|█████████▌| 812/850 [00:31<00:01, 25.83it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  96%|█████████▋| 820/850 [00:31<00:01, 25.97it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  97%|█████████▋| 828/850 [00:31<00:00, 26.11it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  98%|█████████▊| 836/850 [00:31<00:00, 26.26it/s, loss=1.86, v_num=]\n",
      "Epoch 0:  99%|█████████▉| 844/850 [00:31<00:00, 26.41it/s, loss=1.86, v_num=]\n",
      "Epoch 0: 100%|██████████| 850/850 [00:32<00:00, 26.50it/s, loss=1.86, v_num=]\n",
      "Epoch 1:  28%|██▊       | 239/850 [00:11<00:28, 21.50it/s, loss=1.78, v_num=]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import wandb\n",
    "\n",
    "# import pdb\n",
    "\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser.add_argument('-wandb_run_name',\n",
    "                '--wandb_run_name',\n",
    "                help='Name of Wandb Run',\n",
    "                default='run',\n",
    "                type=str)\n",
    "parser.add_argument('-wandb_project_name',\n",
    "                    '--wandb_project_name',\n",
    "                    help='Wandb Project Name',\n",
    "                    default='deep_dream',\n",
    "                    type=str)\n",
    "parser.add_argument('-model_ckpt_path',\n",
    "                    '--model_ckpt_path',\n",
    "                    help='Model Checkpoint Path',\n",
    "                    default='./ckpts/model.ckpt',\n",
    "                    type=str)\n",
    "parser.add_argument('-wandb_log_num_iter',\n",
    "                    '--wandb_log_num_iter',\n",
    "                    help='After how many batches, we will log in training loop',\n",
    "                    default=1,\n",
    "                    type=int)\n",
    "parser.add_argument('-init_ckpt',\n",
    "                    '--init_ckpt',\n",
    "                    help='Initial Ckpt',\n",
    "                    default=None,\n",
    "                    type=str)\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Main function that will perform all the training\"\"\"\n",
    "    # init module\n",
    "    model = BeamClassifier(args)\n",
    "\n",
    "    # Using Wandblogger so that we can log our results to wandb\n",
    "    wandb.init(name=args.wandb_run_name,\n",
    "               project=args.wandb_project_name,\n",
    "               config=vars(args))\n",
    "        \n",
    "    wandb.watch(model)\n",
    "\n",
    "    # most basic trainer, uses good defaults\n",
    "    trainer = Trainer(logger=[], \n",
    "                      gpus=args.gpus, \n",
    "                      max_epochs=args.max_nb_epochs, \n",
    "                      resume_from_checkpoint=args.init_ckpt)\n",
    "#     pdb.set_trace()\n",
    "    trainer.fit(model)\n",
    "    \n",
    "    ckpt_path = os.path.join('./ckpt', f\"{args.wandb_project_name}\", f\"{args.wandb_run_name}.ckpt\")\n",
    "    ckpt_base_path = os.path.dirname(ckpt_path)\n",
    "    trainer.save_checkpoint(ckpt_path)\n",
    "    wandb.save(ckpt_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # auto add args from trainer\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # give the module a chance to add own params\n",
    "    # good practice to define LightningModule speficic params in the module\n",
    "    parser = BeamClassifier.add_model_specific_args(parser)\n",
    "\n",
    "    # parse params\n",
    "    args= parser.parse_args(args_str)\n",
    "\n",
    "    seed_everything(123)\n",
    "\n",
    "    model = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca17d3-c83d-45d2-be81-62161fc85703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a6b02-2fe2-4955-9f9a-694bdd167c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (619)",
   "language": "python",
   "name": "619"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
